{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "animacy_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBJ6euEoReMV"
      },
      "source": [
        "# 1. Initilizations\n",
        "At the beginning, packages neeed to be installed to execute the pipeline. The parameter `install_packages` can be set to True or False to indicate whether the required packages are already installed or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "qW2hnueOVC5g"
      },
      "source": [
        "install_packages = True #@param [\"True\", \"False\"] {type:\"raw\"}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn04gTjzJoL8",
        "outputId": "de6de29b-b0f4-47fe-c459-9f73e6a447d4"
      },
      "source": [
        "if install_packages == True:\n",
        "  !pip3 install tensorflow-gpu==1.15 \n",
        "  !pip3 install -U bert-serving-server bert-serving-client \n",
        "  !pip3 install stanza\n",
        "  !pip install seqeval\n",
        "  !pip3 install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/72/d06017379ad4760dc58781c765376ce4ba5dcf3c08d37032eeefbccf1c51/tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (411.5MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5MB 41kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (3.3.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 47.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.12.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (3.12.4)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.1.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.19.5)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 48.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.32.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.36.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.15) (54.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.3.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.4.1)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=e8d845e4d07e49af8a84db8c405c2ed80ad5ad057474161b3e70db38ddb0dd27\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorboard~=2.4, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorflow-estimator<2.5.0,>=2.4.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, keras-applications, gast, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n",
            "Collecting bert-serving-server\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/bd/cab677bbd0c5fb08b72e468371d2bca6ed9507785739b4656b0b5470d90b/bert_serving_server-1.10.0-py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.4MB/s \n",
            "\u001b[?25hCollecting bert-serving-client\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/09/aae1405378a848b2e87769ad89a43d6d71978c4e15534ca48e82e723a72f/bert_serving_client-1.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: pyzmq>=17.1.0 in /usr/local/lib/python3.7/dist-packages (from bert-serving-server) (22.0.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from bert-serving-server) (1.19.5)\n",
            "Collecting GPUtil>=1.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from bert-serving-server) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from bert-serving-server) (1.15.0)\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-cp37-none-any.whl size=7411 sha256=4eed37af3b240112c7608527ef5868ff75af2c240b3653d55d56d94223ae3781\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil, bert-serving-server, bert-serving-client\n",
            "Successfully installed GPUtil-1.4.0 bert-serving-client-1.10.0 bert-serving-server-1.10.0\n",
            "Collecting stanza\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/ae/a70a58ce6b4e2daad538688806ee0f238dbe601954582a74ea57cde6c532/stanza-1.2-py3-none-any.whl (282kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanza) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from stanza) (1.8.1+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->stanza) (54.2.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n",
            "Installing collected packages: stanza\n",
            "Successfully installed stanza-1.2\n",
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16172 sha256=4ccd97dc5d2ee5e23b2b51b49fd09d9c614eaef35515e2022eb02e5f4e411429\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 7.5MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 36.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 48.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=f399c394798a339f6dc741079f43d9bbd0a7b7140d77fa456e5b1fc2d36800c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysFpJmJdtiLS"
      },
      "source": [
        "## Import required Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkKTn4CwwbuL"
      },
      "source": [
        "import xml.dom.minidom\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import stanza\n",
        "import pickle\n",
        "from bert_serving.client import BertClient\n",
        "from progressbar import ProgressBar\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "from matplotlib import pyplot as plt\n",
        "from seqeval.metrics import f1_score, accuracy_score\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch import nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V5_R0RItpq5"
      },
      "source": [
        "## Mount the Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNwbmzNIxXEw",
        "outputId": "bebb576a-f4f9-4521-ad65-c96e552f151f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkKC3EJQtza4"
      },
      "source": [
        "## Set the Working Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chua9eLRDdzC"
      },
      "source": [
        "working_dir = '/content/drive/My Drive/Colab Notebooks/NLP/'\n",
        "working_dir_extern = r'/content/drive/My\\ Drive/Colab\\ Notebooks/NLP/' # set backslashes befor the whitespaces "
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evtVIAeG2a0A"
      },
      "source": [
        "# Animacy Detection\n",
        "To train an animacy detection classifier (ADC) a training data set is needed containing each single sentence once. It differs from the semantic role labeling, where each sentence need to be contained in the corpus as often as the number of predicates in the respective sentence. The ADC is trained on the Russian Fairytales data set since it is the only corpus for which gold standard animacy annotation is available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-2xpE4kr_mn"
      },
      "source": [
        "## Setting Global Variables\n",
        "\n",
        "At first, the parameter `input_xml_file_dir` need to be set. It defines the input directory for the single stories of the Russian Fairytales data set. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_-VFzPVuYkw",
        "cellView": "form"
      },
      "source": [
        "input_xml_file_dir = \"data/russian_fairytales/xmls/\" #@param {type:\"string\"}"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mzFdDTPtIlw"
      },
      "source": [
        "Since the Russian Fairytales where originally saved in .xml format, they need to be parsed and saved as pandas dataframes. This step is performed by the `scripts/TrainingDataExtraction_AnimacyDetection.py` python script. The step only needs be be executed, if the directory `data/russian_fairytales/parsed_pickles` is empty. The variable `parse_xml_files` defines whether this step is performed or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "WFi4RI7RwfiJ"
      },
      "source": [
        "parse_xml_files = False #@param [\"True\", \"False\"] {type:\"raw\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO6KujL3ERy3"
      },
      "source": [
        "During the implementation process we tested different Bert models, therefore we decided to use the BertClient(). To run it the respective model files need to be downloaded. Since github repositories are restricted to a maximum file size of 100MB, the model could not be added to the git. However, it is automatically downloaded when the variable `download_bert_model` is set to True."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "LL5CvO9IEgQJ"
      },
      "source": [
        "download_bert_model = True #@param [\"True\", \"False\"] {type:\"raw\"}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4Z2vx_1w2C7"
      },
      "source": [
        "The variable `preprocessing_already_performed` needs to be set to determine if word and neighborhood embeddings calculation as well dummy variable creation and other preprocessing steps need to be done.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "8r6j10P_zgmb"
      },
      "source": [
        "preprocessing_already_performed = True #@param [\"True\", \"False\"] {type:\"raw\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj3S6eMDO7tq"
      },
      "source": [
        "The animacy classifier is already pre-trained on the Russian Fairytales data set. The model file can be found under `models/AnimacyDetection_SVM_model.pickle`. If the classifier should be trained again, the variable `retrain_animacy_classifier` can be set to True."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "D47sBYKze6x1"
      },
      "source": [
        "retrain_animacy_classifier = False #@param [\"True\", \"False\"] {type:\"raw\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCYVg9P9RlKi"
      },
      "source": [
        "## Load the Training Data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEE--Ji456d-",
        "outputId": "b308ba2a-666f-4a24-95af-155ea6100b32"
      },
      "source": [
        "path_to_xml_files = working_dir + input_xml_file_dir\n",
        "input_files = [f for f in os.listdir(path_to_xml_files)]\n",
        "output_dir = working_dir + 'data/russian_fairytales/parsed_pickles'\n",
        "print(input_files)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['story1.sty', 'story2.sty', 'story6.sty', 'story5.sty', 'story10.sty', 'story4.sty', 'story3.sty', 'story9.sty', 'story7.sty', 'story12.sty', 'story13.sty', 'story8.sty', 'story15.sty', 'story11.sty', 'story14.sty']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1Lan3jKiGy8"
      },
      "source": [
        "if parse_xml_files == True:\n",
        "  path_to_xml_files_extern = working_dir_extern + input_xml_file_dir\n",
        "  !python scripts/TrainingDataExtraction_AnimacyDetection.py {path_to_xml_files_extern}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34KLVwqirQKA"
      },
      "source": [
        "## Starting BERT Server\n",
        "The Bert server is started using the Bert Base cased model, which is located at `data/bert_model/cased_L-12_H-768_A-12`. The client is started with no pooling strategy to compute word instead of sentence embeddings. The max_seq_length was set to 145 corresponding to the length of the longest tokenized sentence in the Russian Fairytales dataset plus two for the [CLS] and [SEP] token that indicate the start end end of a sentence.<br>\n",
        "The stdout and stderr are forwarded to the file `data/bert_model/log/bert-serving-start_output.log`. The server is finally initialized when the last line of the logfile displays: \"all set, ready to serve request!\". The initialization requires about a minute. The \"!cat logfile\" cell can be reexecuted to check the initialization status of the Bert client."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCl40dEqBsHF",
        "outputId": "5b0d1389-f366-47bb-be62-7e7977f75d29"
      },
      "source": [
        "if download_bert_model == True:\n",
        "  bert_model_download_destination = working_dir_extern + 'data/bert_model/'\n",
        "  !wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip -P {bert_model_download_destination}\n",
        "  downloaded_bert_model_zip = bert_model_download_destination + 'cased_L-12_H-768_A-12.zip'\n",
        "  !unzip {downloaded_bert_model_zip} -d {bert_model_download_destination}\n",
        "  download_bert_model = False"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-31 07:20:16--  https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.189.128, 108.177.97.128, 108.177.125.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.189.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 404261442 (386M) [application/zip]\n",
            "Saving to: ‘/content/drive/My Drive/Colab Notebooks/NLP/test-model-download/cased_L-12_H-768_A-12.zip.1’\n",
            "\n",
            "cased_L-12_H-768_A- 100%[===================>] 385.53M  52.4MB/s    in 7.4s    \n",
            "\n",
            "2021-03-31 07:20:24 (51.8 MB/s) - ‘/content/drive/My Drive/Colab Notebooks/NLP/test-model-download/cased_L-12_H-768_A-12.zip.1’ saved [404261442/404261442]\n",
            "\n",
            "Archive:  /content/drive/My Drive/Colab Notebooks/NLP/test-model-download/cased_L-12_H-768_A-12.zip\n",
            "   creating: /content/drive/My Drive/Colab Notebooks/NLP/test-model-download/cased_L-12_H-768_A-12/\n",
            "  inflating: /content/drive/My Drive/Colab Notebooks/NLP/test-model-download/cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: /content/drive/My Drive/Colab Notebooks/NLP/test-model-download/cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: /content/drive/My Drive/Colab Notebooks/NLP/test-model-download/cased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: /content/drive/My Drive/Colab Notebooks/NLP/test-model-download/cased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: /content/drive/My Drive/Colab Notebooks/NLP/test-model-download/cased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC8i2oHprJDx"
      },
      "source": [
        "if preprocessing_already_performed == False:\n",
        "  log_file = working_dir_extern + 'data/bert_model/log/bert-serving-start_output.log'\n",
        "  bert_model_filepath = working_dir_extern + 'data/bert_model/cased_L-12_H-768_A-12/'\n",
        "  !nohup bert-serving-start -model_dir {bert_model_filepath} -pooling_strategy NONE -max_seq_len 145 -num_worker=4 > {log_file} 2>&1 &"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCZGLlFSrJDz"
      },
      "source": [
        "if preprocessing_already_performed == False:\n",
        "  # check the nohup output file to see if the bert server client is running correctly\n",
        "  log_file = working_dir_extern + 'data/bert_model/log/bert-serving-start_output.log'\n",
        "  !cat {log_file}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8ukpu1nRcAd"
      },
      "source": [
        "client = BertClient() # initialize Bert client"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2DTkSLURcFa"
      },
      "source": [
        "if preprocessing_already_performed == False:\n",
        "  vec = client.encode(['word']) # test the Bert cient\n",
        "  print(vec[0][1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CTg2Ph_5Cn9"
      },
      "source": [
        "## Compute Word Embeddings\n",
        "The word embeddings for the sentences of each story are calculated using the Bert client. The already existing dataframes for each story of the data set are expanded with an additional column containing the calculated word embeddings. Each embedding is a 768-dimensional vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qit0iNlOU-_W"
      },
      "source": [
        "if preprocessing_already_performed == False:\n",
        "  \n",
        "  pbar = ProgressBar() # initialize progress bar\n",
        "\n",
        "  for filename in pbar(input_files): # iterate over input files\n",
        "    filename_df = output_dir + filename[:-4] + '_df.pickle' # define filename for the pickle file that was previously created\n",
        "    output_filename = output_dir + filename[:-4] + '_df.pickle' # set output filename equal to input filename to overwrite the file and save some disk space\n",
        "    df = pd.read_pickle(filename_df) # read file\n",
        "    embedding_list = []\n",
        "\n",
        "    for index, row in df.iterrows(): # iterate over rows of the df\n",
        "      token = row['token']\n",
        "      embedding = client.encode([token])[0][1] # compute Bert embedding for each single token\n",
        "      embedding_list.append(embedding)\n",
        "\n",
        "    df['word_embedding'] = embedding_list # insert a new column with the word embeddings to the df\n",
        "    df.to_pickle(output_filename) # save the updated df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPJclu027LYx"
      },
      "source": [
        "## Compute Neighborhood Embeddings\n",
        "As an additional input feature for the animacy detection classifier a so-called neighborhood embedding is computed. The animacy of a word is not only based on the word itself, because a normally inanimate word can, for example, become a animate entity through a certain preceding adjective (e.g. a talking tree).<br>\n",
        "Therefore a neighborhood embedding is calculated by adding up the word embedding vectors of the three tokens before and three tokens after the target word (excluding the target). Again, the already existing dataframes for each story of the data set are expanded with an additional column for the neighborhood embeddings.<br><br>\n",
        "The idea and window size originate from :<br>\n",
        "Jahan, Labiba, Geeticka Chauhan, and Mark A. Finlayson. \"A new approach to animacy detection.\" Proceedings of the 27th International Conference on Computational Linguistics. 2018."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdpknPPphJnp"
      },
      "source": [
        "if preprocessing_already_performed == False:\n",
        "\n",
        "  pbar = ProgressBar() # initialize progress bar\n",
        "  neighborhood_threshhold = 3 # window size\n",
        "\n",
        "  for filename in pbar(input_files): # iterate over input files\n",
        "    filename_df = output_dir + filename[:-4] + '_df.pickle' # define filename for the pickle file that was previously stored\n",
        "    output_filename = output_dir + filename[:-4] + '_df.pickle' # set output filename equal to input filename to overwrite the file and save some disk space\n",
        "    df = pd.read_pickle(filename_df) # read file\n",
        "    nrows = len(df.index)\n",
        "    neighborhood_embedding_list = []\n",
        "\n",
        "    for index, row in df.iterrows(): # iterate over rows of the df\n",
        "      neighborhood_embedding = [0]*768 # initialize neighborhood embeddings array with 768 dimensions (size of Bert embedding)\n",
        "\n",
        "      if index == 0: # special case for the first word of a text\n",
        "        for i in range(1, neighborhood_threshhold+1):\n",
        "          we_index = index + i\n",
        "          vec = df.at[we_index, 'word_embedding']\n",
        "          neighborhood_embedding = neighborhood_embedding + vec\n",
        "\n",
        "      elif index == 1: # special case for the second word of a text\n",
        "        for i in range(-1, neighborhood_threshhold+1):\n",
        "          if i != 0: # exclude the current embedding\n",
        "            we_index = index + i\n",
        "            vec = df.at[we_index, 'word_embedding']\n",
        "            neighborhood_embedding = neighborhood_embedding + vec\n",
        "\n",
        "      elif index == 2: # special case for the third word of a text\n",
        "        for i in range(-2, neighborhood_threshhold+1):\n",
        "          if i != 0: # exclude the current embedding\n",
        "            we_index = index + i\n",
        "            vec = df.at[we_index, 'word_embedding']\n",
        "            neighborhood_embedding = neighborhood_embedding + vec\n",
        "\n",
        "      elif index == nrows-3: # special case for the third last word of a text\n",
        "        for i in range(-3, neighborhood_threshhold):\n",
        "          if i != 0: # exclude the current embedding\n",
        "            we_index = index + i\n",
        "            vec = df.at[we_index, 'word_embedding']\n",
        "            neighborhood_embedding = neighborhood_embedding + vec\n",
        "\n",
        "      elif index == nrows-2: # special case for the next-to-last word of a text\n",
        "        for i in range(-3, neighborhood_threshhold-1):\n",
        "          if i != 0: # exclude the current embedding\n",
        "            we_index = index + i\n",
        "            vec = df.at[we_index, 'word_embedding']\n",
        "            neighborhood_embedding = neighborhood_embedding + vec\n",
        "\n",
        "      elif index == nrows-1: # special case for the last word of a text\n",
        "        for i in range(-3, neighborhood_threshhold-2):\n",
        "          if i != 0: # exclude the current embedding\n",
        "            we_index = index + i\n",
        "            vec = df.at[we_index, 'word_embedding']\n",
        "            neighborhood_embedding = neighborhood_embedding + vec\n",
        "\n",
        "      else: # normal case (current token has three other token before and after it)\n",
        "        for i in range(-3, neighborhood_threshhold+1):\n",
        "          if i != 0: # exclude the current embedding\n",
        "            we_index = index + i\n",
        "            vec = df.at[we_index, 'word_embedding']\n",
        "            neighborhood_embedding = neighborhood_embedding + vec    \n",
        "\n",
        "      neighborhood_embedding_list.append(neighborhood_embedding)\n",
        "\n",
        "    df['neighborhood_embedding'] = neighborhood_embedding_list # insert a new column with the neighborhood embeddings to the df\n",
        "    df.to_pickle(output_filename) # save the updated df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StK7drXpU7sf"
      },
      "source": [
        "## Create the Animacy Classifier (SVM)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5s6oMwFAG5K"
      },
      "source": [
        "### Preprocess the Data\n",
        "To build a classifier on the training data several preprocessing steps need to be performed: \n",
        "- Concatenation of the single dataframes of each story to one big dataframe\n",
        "- Creation of dummy variables for the Part-of-speech-Tags\n",
        "- Creation of dummy variables for the word embeddings\n",
        "- Creation of dummy variables for the neighborhood embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1UaLzXPKWoV"
      },
      "source": [
        "if retrain_animacy_classifier == True:\n",
        "\n",
        "  df = pd.DataFrame(columns=['id', 'token', 'len', 'off', 'animacy', 'pos', 'word_embedding', 'neighborhood_embedding']) # initialize empty df with the same column set\n",
        "  embedding_size = 768 # specify the embedding size\n",
        "\n",
        "  # concatenate the rows of all created df's\n",
        "  for j in range(1, len(input_files)): \n",
        "    filename = output_dir + 'story' + str(j) + '_df.pickle'\n",
        "    df_new = pd.read_pickle(filename)\n",
        "    df = pd.concat([df, df_new], ignore_index=True)\n",
        "\n",
        "  df_pos_dummy = pd.get_dummies(df['pos']) # dummy variables for the pos tagger column\n",
        "  df = pd.concat([df, df_pos_dummy], axis=1) # concatenate the new columns for the dummy variables to the df\n",
        "\n",
        "  # dummy variables for embedding columns (word embedding and neighborhood embedding)\n",
        "  we_dummy = []\n",
        "  ne_dummy = []\n",
        "  for index, row in df.iterrows(): # iterate over rows of the df containing ALL tokens of the training data\n",
        "    we = row['word_embedding']\n",
        "    ne = row['neighborhood_embedding']\n",
        "    row_vector_we = []\n",
        "    row_vector_ne = []\n",
        "    for count, val in enumerate(we): # iterate over word embedding\n",
        "      row_vector_we.append(val)\n",
        "    for count, val in enumerate(ne): # iterate over neighborhood embedding\n",
        "      row_vector_ne.append(val)\n",
        "    we_dummy.append(row_vector_we)\n",
        "    ne_dummy.append(row_vector_ne)\n",
        "    #change the animacy column\n",
        "    if row['animacy'] == True:\n",
        "      df.at[index, 'animacy'] = 1\n",
        "    if row['animacy'] != True:\n",
        "      df.at[index, 'animacy'] = 0 # was None before\n",
        "\n",
        "  # create names for the embedding dummy variables\n",
        "  we_dummy_names = []\n",
        "  ne_dummy_names = []\n",
        "  for i in range(0, embedding_size): \n",
        "    we_dummy_names.append('we_' + str(i))\n",
        "    ne_dummy_names.append('ne_' + str(i))\n",
        "\n",
        "  df_we_dummy = pd.DataFrame(data=we_dummy, columns=we_dummy_names) # create df for word embedding dummy variables\n",
        "  df_ne_dummy = pd.DataFrame(data=ne_dummy, columns=ne_dummy_names) # create df for neighborhood embedding dummy variables\n",
        "\n",
        "  df = pd.concat([df, df_we_dummy, df_ne_dummy], axis=1) # concatenate the new columns for the dummy variables to the original df\n",
        "  df = df.drop(['id', 'token', 'len', 'off', 'pos', 'predicates', 'semrols', 'word_embedding', 'neighborhood_embedding'], axis=1) # drop all columns that are not used in the SVM classification\n",
        "\n",
        "  filename = working_dir + 'data/animacy_detection/output/AnimacyDetection_SVM_input.pickle'\n",
        "  df.to_pickle(filename) # save the df on drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGmu67DzLF1H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "16a74de3-9d49-41bb-f62f-a31e0cf95d53"
      },
      "source": [
        "if retrain_animacy_classifier == True:\n",
        "\n",
        "  # display the input training data for the animacy classifier after the preprocessing\n",
        "  preprocessed_svm_input_file = working_dir + 'data/animacy_detection/output/AnimacyDetection_SVM_input.pickle'\n",
        "  df_show = pd.read_pickle(preprocessed_svm_input_file)\n",
        "  df_show"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>animacy</th>\n",
              "      <th>''</th>\n",
              "      <th>(</th>\n",
              "      <th>)</th>\n",
              "      <th>,</th>\n",
              "      <th>.</th>\n",
              "      <th>:</th>\n",
              "      <th>CC</th>\n",
              "      <th>CD</th>\n",
              "      <th>DT</th>\n",
              "      <th>EX</th>\n",
              "      <th>IN</th>\n",
              "      <th>JJ</th>\n",
              "      <th>JJR</th>\n",
              "      <th>JJS</th>\n",
              "      <th>MD</th>\n",
              "      <th>NN</th>\n",
              "      <th>NNP</th>\n",
              "      <th>NNPS</th>\n",
              "      <th>NNS</th>\n",
              "      <th>PDT</th>\n",
              "      <th>POS</th>\n",
              "      <th>PRP</th>\n",
              "      <th>PRP$</th>\n",
              "      <th>RB</th>\n",
              "      <th>RBR</th>\n",
              "      <th>RBS</th>\n",
              "      <th>RP</th>\n",
              "      <th>TO</th>\n",
              "      <th>UH</th>\n",
              "      <th>VB</th>\n",
              "      <th>VB(aux)</th>\n",
              "      <th>VBD</th>\n",
              "      <th>VBD(aux)</th>\n",
              "      <th>VBG</th>\n",
              "      <th>VBG(aux)</th>\n",
              "      <th>VBN</th>\n",
              "      <th>VBN(aux)</th>\n",
              "      <th>VBP</th>\n",
              "      <th>VBP(aux)</th>\n",
              "      <th>...</th>\n",
              "      <th>ne_728</th>\n",
              "      <th>ne_729</th>\n",
              "      <th>ne_730</th>\n",
              "      <th>ne_731</th>\n",
              "      <th>ne_732</th>\n",
              "      <th>ne_733</th>\n",
              "      <th>ne_734</th>\n",
              "      <th>ne_735</th>\n",
              "      <th>ne_736</th>\n",
              "      <th>ne_737</th>\n",
              "      <th>ne_738</th>\n",
              "      <th>ne_739</th>\n",
              "      <th>ne_740</th>\n",
              "      <th>ne_741</th>\n",
              "      <th>ne_742</th>\n",
              "      <th>ne_743</th>\n",
              "      <th>ne_744</th>\n",
              "      <th>ne_745</th>\n",
              "      <th>ne_746</th>\n",
              "      <th>ne_747</th>\n",
              "      <th>ne_748</th>\n",
              "      <th>ne_749</th>\n",
              "      <th>ne_750</th>\n",
              "      <th>ne_751</th>\n",
              "      <th>ne_752</th>\n",
              "      <th>ne_753</th>\n",
              "      <th>ne_754</th>\n",
              "      <th>ne_755</th>\n",
              "      <th>ne_756</th>\n",
              "      <th>ne_757</th>\n",
              "      <th>ne_758</th>\n",
              "      <th>ne_759</th>\n",
              "      <th>ne_760</th>\n",
              "      <th>ne_761</th>\n",
              "      <th>ne_762</th>\n",
              "      <th>ne_763</th>\n",
              "      <th>ne_764</th>\n",
              "      <th>ne_765</th>\n",
              "      <th>ne_766</th>\n",
              "      <th>ne_767</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.918093</td>\n",
              "      <td>0.086954</td>\n",
              "      <td>-0.583026</td>\n",
              "      <td>1.258105</td>\n",
              "      <td>-0.206404</td>\n",
              "      <td>-1.218740</td>\n",
              "      <td>-0.074535</td>\n",
              "      <td>0.751082</td>\n",
              "      <td>-0.301104</td>\n",
              "      <td>1.329131</td>\n",
              "      <td>-0.373164</td>\n",
              "      <td>-3.470462</td>\n",
              "      <td>0.228836</td>\n",
              "      <td>1.883012</td>\n",
              "      <td>-1.036766</td>\n",
              "      <td>-2.034585</td>\n",
              "      <td>0.324802</td>\n",
              "      <td>1.980626</td>\n",
              "      <td>1.729677</td>\n",
              "      <td>-0.575444</td>\n",
              "      <td>2.647425</td>\n",
              "      <td>-0.437884</td>\n",
              "      <td>1.379815</td>\n",
              "      <td>-1.225402</td>\n",
              "      <td>2.439398</td>\n",
              "      <td>1.909036</td>\n",
              "      <td>-1.366847</td>\n",
              "      <td>0.484081</td>\n",
              "      <td>0.851856</td>\n",
              "      <td>0.720229</td>\n",
              "      <td>1.436187</td>\n",
              "      <td>1.039622</td>\n",
              "      <td>-1.592968</td>\n",
              "      <td>-0.145159</td>\n",
              "      <td>0.751654</td>\n",
              "      <td>0.583159</td>\n",
              "      <td>1.397123</td>\n",
              "      <td>-3.002742</td>\n",
              "      <td>1.729545</td>\n",
              "      <td>-1.298351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.422045</td>\n",
              "      <td>0.033532</td>\n",
              "      <td>-2.257727</td>\n",
              "      <td>2.703270</td>\n",
              "      <td>0.952274</td>\n",
              "      <td>-0.832260</td>\n",
              "      <td>0.096600</td>\n",
              "      <td>0.488836</td>\n",
              "      <td>0.839612</td>\n",
              "      <td>2.128399</td>\n",
              "      <td>-0.815296</td>\n",
              "      <td>-3.693896</td>\n",
              "      <td>-0.568984</td>\n",
              "      <td>2.601876</td>\n",
              "      <td>-0.060462</td>\n",
              "      <td>-1.689525</td>\n",
              "      <td>-0.982853</td>\n",
              "      <td>1.894402</td>\n",
              "      <td>2.023593</td>\n",
              "      <td>-1.267332</td>\n",
              "      <td>2.065489</td>\n",
              "      <td>-0.784187</td>\n",
              "      <td>1.535921</td>\n",
              "      <td>-1.409366</td>\n",
              "      <td>2.199467</td>\n",
              "      <td>1.560350</td>\n",
              "      <td>-2.603150</td>\n",
              "      <td>-0.115937</td>\n",
              "      <td>1.851276</td>\n",
              "      <td>0.674540</td>\n",
              "      <td>0.801450</td>\n",
              "      <td>1.490037</td>\n",
              "      <td>-2.478864</td>\n",
              "      <td>1.266570</td>\n",
              "      <td>0.471990</td>\n",
              "      <td>-0.776899</td>\n",
              "      <td>1.482090</td>\n",
              "      <td>-1.018828</td>\n",
              "      <td>2.221318</td>\n",
              "      <td>-0.123411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.144797</td>\n",
              "      <td>-0.598883</td>\n",
              "      <td>-1.390926</td>\n",
              "      <td>2.748091</td>\n",
              "      <td>0.765653</td>\n",
              "      <td>-1.655028</td>\n",
              "      <td>0.060305</td>\n",
              "      <td>-0.010772</td>\n",
              "      <td>0.110657</td>\n",
              "      <td>2.312299</td>\n",
              "      <td>-0.713801</td>\n",
              "      <td>-5.904859</td>\n",
              "      <td>0.076504</td>\n",
              "      <td>2.564359</td>\n",
              "      <td>-0.236705</td>\n",
              "      <td>-2.472365</td>\n",
              "      <td>-0.033515</td>\n",
              "      <td>4.183627</td>\n",
              "      <td>1.622482</td>\n",
              "      <td>0.318475</td>\n",
              "      <td>3.274540</td>\n",
              "      <td>-1.240932</td>\n",
              "      <td>0.255007</td>\n",
              "      <td>-0.061527</td>\n",
              "      <td>1.980274</td>\n",
              "      <td>0.790543</td>\n",
              "      <td>-3.486110</td>\n",
              "      <td>1.081771</td>\n",
              "      <td>2.021750</td>\n",
              "      <td>1.696872</td>\n",
              "      <td>1.518320</td>\n",
              "      <td>1.839940</td>\n",
              "      <td>-3.680840</td>\n",
              "      <td>1.343347</td>\n",
              "      <td>1.281339</td>\n",
              "      <td>-0.008658</td>\n",
              "      <td>3.064608</td>\n",
              "      <td>-1.675531</td>\n",
              "      <td>1.613031</td>\n",
              "      <td>0.857189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.799067</td>\n",
              "      <td>0.581621</td>\n",
              "      <td>-1.097689</td>\n",
              "      <td>3.133466</td>\n",
              "      <td>1.359195</td>\n",
              "      <td>-1.004535</td>\n",
              "      <td>1.134536</td>\n",
              "      <td>0.324329</td>\n",
              "      <td>0.216892</td>\n",
              "      <td>2.462036</td>\n",
              "      <td>-1.054180</td>\n",
              "      <td>-7.573497</td>\n",
              "      <td>-1.402986</td>\n",
              "      <td>3.237759</td>\n",
              "      <td>-0.892228</td>\n",
              "      <td>-2.707234</td>\n",
              "      <td>-0.441506</td>\n",
              "      <td>4.517787</td>\n",
              "      <td>2.618826</td>\n",
              "      <td>-0.236844</td>\n",
              "      <td>2.963569</td>\n",
              "      <td>-1.213316</td>\n",
              "      <td>-0.172033</td>\n",
              "      <td>-0.427601</td>\n",
              "      <td>2.979504</td>\n",
              "      <td>1.799285</td>\n",
              "      <td>-3.758044</td>\n",
              "      <td>2.023717</td>\n",
              "      <td>1.677245</td>\n",
              "      <td>1.355356</td>\n",
              "      <td>2.020262</td>\n",
              "      <td>2.272515</td>\n",
              "      <td>-3.676766</td>\n",
              "      <td>2.006212</td>\n",
              "      <td>1.425031</td>\n",
              "      <td>-0.864373</td>\n",
              "      <td>3.070915</td>\n",
              "      <td>-1.379507</td>\n",
              "      <td>2.210242</td>\n",
              "      <td>1.237369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.972254</td>\n",
              "      <td>-0.077723</td>\n",
              "      <td>-0.485441</td>\n",
              "      <td>2.249425</td>\n",
              "      <td>-0.622157</td>\n",
              "      <td>-1.010898</td>\n",
              "      <td>0.960304</td>\n",
              "      <td>-0.223034</td>\n",
              "      <td>-1.695045</td>\n",
              "      <td>3.037597</td>\n",
              "      <td>-1.424008</td>\n",
              "      <td>-7.006863</td>\n",
              "      <td>-0.433105</td>\n",
              "      <td>3.238289</td>\n",
              "      <td>-1.546361</td>\n",
              "      <td>-2.813712</td>\n",
              "      <td>0.835603</td>\n",
              "      <td>4.979222</td>\n",
              "      <td>3.319717</td>\n",
              "      <td>-1.938824</td>\n",
              "      <td>3.164280</td>\n",
              "      <td>-1.233637</td>\n",
              "      <td>0.289878</td>\n",
              "      <td>-1.405635</td>\n",
              "      <td>5.439663</td>\n",
              "      <td>2.465936</td>\n",
              "      <td>-3.136951</td>\n",
              "      <td>1.706111</td>\n",
              "      <td>2.717264</td>\n",
              "      <td>1.694358</td>\n",
              "      <td>2.967423</td>\n",
              "      <td>0.792259</td>\n",
              "      <td>-3.882027</td>\n",
              "      <td>2.151967</td>\n",
              "      <td>1.546334</td>\n",
              "      <td>1.033038</td>\n",
              "      <td>1.178526</td>\n",
              "      <td>-3.834439</td>\n",
              "      <td>1.276596</td>\n",
              "      <td>-0.185428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23286</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.479945</td>\n",
              "      <td>-2.582466</td>\n",
              "      <td>0.489591</td>\n",
              "      <td>0.999384</td>\n",
              "      <td>1.255741</td>\n",
              "      <td>-2.884831</td>\n",
              "      <td>-0.069066</td>\n",
              "      <td>2.893879</td>\n",
              "      <td>-1.337272</td>\n",
              "      <td>3.892410</td>\n",
              "      <td>-0.318075</td>\n",
              "      <td>-7.105108</td>\n",
              "      <td>2.402276</td>\n",
              "      <td>2.351266</td>\n",
              "      <td>-2.159379</td>\n",
              "      <td>-4.621271</td>\n",
              "      <td>1.636343</td>\n",
              "      <td>3.469443</td>\n",
              "      <td>4.893570</td>\n",
              "      <td>-1.581366</td>\n",
              "      <td>4.611453</td>\n",
              "      <td>-0.300243</td>\n",
              "      <td>0.994488</td>\n",
              "      <td>-3.195226</td>\n",
              "      <td>6.898794</td>\n",
              "      <td>2.129744</td>\n",
              "      <td>-3.599612</td>\n",
              "      <td>4.354463</td>\n",
              "      <td>-1.031533</td>\n",
              "      <td>2.018215</td>\n",
              "      <td>4.796746</td>\n",
              "      <td>0.501574</td>\n",
              "      <td>-2.238529</td>\n",
              "      <td>3.713053</td>\n",
              "      <td>2.439788</td>\n",
              "      <td>0.434404</td>\n",
              "      <td>0.611327</td>\n",
              "      <td>-2.657500</td>\n",
              "      <td>2.440448</td>\n",
              "      <td>0.834722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23287</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.308941</td>\n",
              "      <td>-1.548630</td>\n",
              "      <td>0.874355</td>\n",
              "      <td>0.252812</td>\n",
              "      <td>1.569754</td>\n",
              "      <td>-1.472999</td>\n",
              "      <td>0.944953</td>\n",
              "      <td>2.191914</td>\n",
              "      <td>-2.677643</td>\n",
              "      <td>3.341186</td>\n",
              "      <td>0.141481</td>\n",
              "      <td>-7.564830</td>\n",
              "      <td>1.380130</td>\n",
              "      <td>2.474999</td>\n",
              "      <td>-2.020688</td>\n",
              "      <td>-3.624573</td>\n",
              "      <td>0.249854</td>\n",
              "      <td>3.352085</td>\n",
              "      <td>3.867459</td>\n",
              "      <td>-2.979676</td>\n",
              "      <td>3.146408</td>\n",
              "      <td>-0.469567</td>\n",
              "      <td>1.064546</td>\n",
              "      <td>-2.694563</td>\n",
              "      <td>6.800439</td>\n",
              "      <td>2.334458</td>\n",
              "      <td>-4.483286</td>\n",
              "      <td>2.497017</td>\n",
              "      <td>0.619594</td>\n",
              "      <td>1.252880</td>\n",
              "      <td>3.143400</td>\n",
              "      <td>-0.217112</td>\n",
              "      <td>-1.997694</td>\n",
              "      <td>3.740375</td>\n",
              "      <td>2.710693</td>\n",
              "      <td>1.607655</td>\n",
              "      <td>1.906522</td>\n",
              "      <td>-1.139683</td>\n",
              "      <td>2.614963</td>\n",
              "      <td>0.653531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23288</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.025663</td>\n",
              "      <td>-0.578275</td>\n",
              "      <td>0.199421</td>\n",
              "      <td>0.472389</td>\n",
              "      <td>0.675119</td>\n",
              "      <td>-1.297758</td>\n",
              "      <td>1.185802</td>\n",
              "      <td>-0.335990</td>\n",
              "      <td>-1.923302</td>\n",
              "      <td>1.879944</td>\n",
              "      <td>0.036961</td>\n",
              "      <td>-7.659544</td>\n",
              "      <td>0.906275</td>\n",
              "      <td>3.798326</td>\n",
              "      <td>-1.878915</td>\n",
              "      <td>-3.705195</td>\n",
              "      <td>1.386115</td>\n",
              "      <td>2.362198</td>\n",
              "      <td>2.622586</td>\n",
              "      <td>-2.802380</td>\n",
              "      <td>4.283153</td>\n",
              "      <td>0.129265</td>\n",
              "      <td>1.292023</td>\n",
              "      <td>-2.469410</td>\n",
              "      <td>5.859423</td>\n",
              "      <td>2.505576</td>\n",
              "      <td>-4.658539</td>\n",
              "      <td>1.976933</td>\n",
              "      <td>0.706546</td>\n",
              "      <td>1.534950</td>\n",
              "      <td>1.298239</td>\n",
              "      <td>1.246978</td>\n",
              "      <td>-2.765628</td>\n",
              "      <td>1.273308</td>\n",
              "      <td>2.883018</td>\n",
              "      <td>2.649454</td>\n",
              "      <td>0.804991</td>\n",
              "      <td>-1.369980</td>\n",
              "      <td>2.195318</td>\n",
              "      <td>0.616243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23289</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.729320</td>\n",
              "      <td>-0.633443</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.297637</td>\n",
              "      <td>1.293739</td>\n",
              "      <td>-1.910581</td>\n",
              "      <td>0.549039</td>\n",
              "      <td>0.141379</td>\n",
              "      <td>-3.057657</td>\n",
              "      <td>2.316450</td>\n",
              "      <td>-0.574438</td>\n",
              "      <td>-5.485801</td>\n",
              "      <td>0.695491</td>\n",
              "      <td>2.279943</td>\n",
              "      <td>-0.789255</td>\n",
              "      <td>-3.852139</td>\n",
              "      <td>0.421317</td>\n",
              "      <td>2.516200</td>\n",
              "      <td>2.415451</td>\n",
              "      <td>-2.181823</td>\n",
              "      <td>2.185555</td>\n",
              "      <td>-1.376667</td>\n",
              "      <td>1.203388</td>\n",
              "      <td>-1.007329</td>\n",
              "      <td>4.598256</td>\n",
              "      <td>0.265917</td>\n",
              "      <td>-3.020582</td>\n",
              "      <td>0.589419</td>\n",
              "      <td>1.853697</td>\n",
              "      <td>1.193842</td>\n",
              "      <td>1.349789</td>\n",
              "      <td>0.928071</td>\n",
              "      <td>-2.898418</td>\n",
              "      <td>2.301269</td>\n",
              "      <td>1.321291</td>\n",
              "      <td>1.174569</td>\n",
              "      <td>1.233121</td>\n",
              "      <td>-1.251832</td>\n",
              "      <td>2.011714</td>\n",
              "      <td>0.255001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23290</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.173653</td>\n",
              "      <td>-0.920465</td>\n",
              "      <td>0.479049</td>\n",
              "      <td>0.236945</td>\n",
              "      <td>0.771411</td>\n",
              "      <td>-1.747848</td>\n",
              "      <td>-0.152332</td>\n",
              "      <td>0.917395</td>\n",
              "      <td>-1.085071</td>\n",
              "      <td>2.115662</td>\n",
              "      <td>0.094749</td>\n",
              "      <td>-4.132426</td>\n",
              "      <td>1.467878</td>\n",
              "      <td>1.599086</td>\n",
              "      <td>-1.033710</td>\n",
              "      <td>-2.647454</td>\n",
              "      <td>1.978543</td>\n",
              "      <td>1.460816</td>\n",
              "      <td>2.703813</td>\n",
              "      <td>-1.340674</td>\n",
              "      <td>2.571050</td>\n",
              "      <td>-0.029971</td>\n",
              "      <td>0.687927</td>\n",
              "      <td>-1.128203</td>\n",
              "      <td>3.241337</td>\n",
              "      <td>0.612016</td>\n",
              "      <td>-1.822178</td>\n",
              "      <td>2.114750</td>\n",
              "      <td>-0.771522</td>\n",
              "      <td>1.345009</td>\n",
              "      <td>2.841093</td>\n",
              "      <td>0.817065</td>\n",
              "      <td>-1.848576</td>\n",
              "      <td>0.697475</td>\n",
              "      <td>1.085865</td>\n",
              "      <td>0.713238</td>\n",
              "      <td>0.069731</td>\n",
              "      <td>-1.520494</td>\n",
              "      <td>0.805097</td>\n",
              "      <td>0.389253</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>23291 rows × 1583 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      animacy  ''  (  )  ,  ...    ne_763    ne_764    ne_765    ne_766    ne_767\n",
              "0           0   0  0  0  0  ...  0.583159  1.397123 -3.002742  1.729545 -1.298351\n",
              "1           1   0  0  0  0  ... -0.776899  1.482090 -1.018828  2.221318 -0.123411\n",
              "2           0   0  0  0  0  ... -0.008658  3.064608 -1.675531  1.613031  0.857189\n",
              "3           0   0  0  0  0  ... -0.864373  3.070915 -1.379507  2.210242  1.237369\n",
              "4           0   0  0  0  0  ...  1.033038  1.178526 -3.834439  1.276596 -0.185428\n",
              "...       ...  .. .. .. ..  ...       ...       ...       ...       ...       ...\n",
              "23286       0   0  0  0  0  ...  0.434404  0.611327 -2.657500  2.440448  0.834722\n",
              "23287       1   0  0  0  0  ...  1.607655  1.906522 -1.139683  2.614963  0.653531\n",
              "23288       1   0  0  0  0  ...  2.649454  0.804991 -1.369980  2.195318  0.616243\n",
              "23289       0   0  0  0  0  ...  1.174569  1.233121 -1.251832  2.011714  0.255001\n",
              "23290       0   0  0  0  0  ...  0.713238  0.069731 -1.520494  0.805097  0.389253\n",
              "\n",
              "[23291 rows x 1583 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsbXS_ytAdVI"
      },
      "source": [
        "### Train, Test and Evaluate the SVM Classifier\n",
        "In the following, the classifier is constructed and evaluted using 10-fold crossvalidation and a 90:10 train-test split. For the evaluation of the classification results the accuracy as well as the precision, recall, fscore, support and confusion matrix is reported for the classification of the animate and inanimate class respectively. The evaluation results are saved to file and can be found in the `data/animacy_detecton/output/` directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn-qSU5nU7yE"
      },
      "source": [
        "if retrain_animacy_classifier == True:\n",
        "\n",
        "  filename = working_dir + 'data/animacy_detection/output/AnimacyDetection_SVM_input.pickle' # load preprocessed input file\n",
        "  df = pd.read_pickle(filename)\n",
        "  \n",
        "  Y = np.array(df['animacy']) # target column\n",
        "  Y = Y.astype('int')\n",
        "  X = df.drop(['animacy'], axis=1) # input features \n",
        "  \n",
        "  # initialization for the classifier evaluation\n",
        "  accuracy_list = []\n",
        "  precision_list_0 = []\n",
        "  precision_list_1 = []\n",
        "  recall_list_0 = []\n",
        "  recall_list_1 = []\n",
        "  fscore_list_0 = []\n",
        "  fscore_list_1 = []\n",
        "  support_list_0 = []\n",
        "  support_list_1 = []\n",
        "  CM_list = []\n",
        "  \n",
        "  n_fold_crossvalidation = 10 # number of cross-validations\n",
        "  \n",
        "  for i in range(0, n_fold_crossvalidation):\n",
        "  \n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1) # split data into 90% training set and 10% test set\n",
        "    \n",
        "    # training\n",
        "    svclassifier = SVC(kernel='linear')\n",
        "    svclassifier.fit(X_train, Y_train)\n",
        "  \n",
        "    # prediction\n",
        "    Y_pred = svclassifier.predict(X_test) \n",
        "  \n",
        "    # elaluation\n",
        "    CM = confusion_matrix(Y_test, Y_pred)\n",
        "    CM_list.append(CM)\n",
        "  \n",
        "    acc = accuracy_score(Y_test, Y_pred)\n",
        "    accuracy_list.append(acc)\n",
        "  \n",
        "    precision,recall,fscore,support=score(Y_test, Y_pred)\n",
        "    precision_list_0.append(precision[0])\n",
        "    recall_list_0.append(recall[0])\n",
        "    fscore_list_0.append(fscore[0])\n",
        "    support_list_0.append(support[0])\n",
        "    precision_list_1.append(precision[1])\n",
        "    recall_list_1.append(recall[1])\n",
        "    fscore_list_1.append(fscore[1])\n",
        "    support_list_1.append(support[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDkDutVSBpj6"
      },
      "source": [
        "**Results for each CV-Run**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93Sz41qpfNUt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "dac568dc-00a7-4dcf-b26e-d19e4bb9eb5b"
      },
      "source": [
        "if retrain_animacy_classifier == True:\n",
        "\n",
        "  data = {'accuracy': accuracy_list, 'precision_inanimate': precision_list_0, 'precision_animate': precision_list_1, 'recall_inanimate': recall_list_0, 'recall_animate': recall_list_1, 'f1-score_inanimate': fscore_list_0, 'f1-score_animate': fscore_list_1, 'support_inanimate': support_list_0, 'support_animate': support_list_1, 'confusion_matrix': CM_list}\n",
        "  df_evaluation = pd.DataFrame(data, columns=['accuracy', 'precision_inanimate', 'precision_animate', 'recall_inanimate', 'recall_animate', 'f1-score_inanimate', 'f1-score_animate', 'support_inanimate', 'support_animate', 'confusion_matrix'])\n",
        "  \n",
        "  filename = working_dir + 'data/animacy_detection/output/' + 'AnimacyDetection_SVM_evaluation_' + str(n_fold_crossvalidation) + 'foldCV.pickle'\n",
        "  df_evaluation.to_pickle(filename)\n",
        "  \n",
        "  df_evaluation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_inanimate</th>\n",
              "      <th>precision_animate</th>\n",
              "      <th>recall_inanimate</th>\n",
              "      <th>recall_animate</th>\n",
              "      <th>f1-score_inanimate</th>\n",
              "      <th>f1-score_animate</th>\n",
              "      <th>support_inanimate</th>\n",
              "      <th>support_animate</th>\n",
              "      <th>confusion_matrix</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.969099</td>\n",
              "      <td>0.982106</td>\n",
              "      <td>0.901070</td>\n",
              "      <td>0.981103</td>\n",
              "      <td>0.905914</td>\n",
              "      <td>0.981604</td>\n",
              "      <td>0.903485</td>\n",
              "      <td>1958</td>\n",
              "      <td>372</td>\n",
              "      <td>[[1921, 37], [35, 337]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.971674</td>\n",
              "      <td>0.980493</td>\n",
              "      <td>0.926702</td>\n",
              "      <td>0.985552</td>\n",
              "      <td>0.903061</td>\n",
              "      <td>0.983016</td>\n",
              "      <td>0.914729</td>\n",
              "      <td>1938</td>\n",
              "      <td>392</td>\n",
              "      <td>[[1910, 28], [38, 354]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.969099</td>\n",
              "      <td>0.983497</td>\n",
              "      <td>0.897698</td>\n",
              "      <td>0.979456</td>\n",
              "      <td>0.916449</td>\n",
              "      <td>0.981472</td>\n",
              "      <td>0.906977</td>\n",
              "      <td>1947</td>\n",
              "      <td>383</td>\n",
              "      <td>[[1907, 40], [32, 351]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.967382</td>\n",
              "      <td>0.984424</td>\n",
              "      <td>0.886139</td>\n",
              "      <td>0.976313</td>\n",
              "      <td>0.922680</td>\n",
              "      <td>0.980352</td>\n",
              "      <td>0.904040</td>\n",
              "      <td>1942</td>\n",
              "      <td>388</td>\n",
              "      <td>[[1896, 46], [30, 358]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.966953</td>\n",
              "      <td>0.975971</td>\n",
              "      <td>0.919786</td>\n",
              "      <td>0.984528</td>\n",
              "      <td>0.879795</td>\n",
              "      <td>0.980231</td>\n",
              "      <td>0.899346</td>\n",
              "      <td>1939</td>\n",
              "      <td>391</td>\n",
              "      <td>[[1909, 30], [47, 344]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.965665</td>\n",
              "      <td>0.976911</td>\n",
              "      <td>0.908136</td>\n",
              "      <td>0.981949</td>\n",
              "      <td>0.884910</td>\n",
              "      <td>0.979424</td>\n",
              "      <td>0.896373</td>\n",
              "      <td>1939</td>\n",
              "      <td>391</td>\n",
              "      <td>[[1904, 35], [45, 346]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.969528</td>\n",
              "      <td>0.983428</td>\n",
              "      <td>0.902256</td>\n",
              "      <td>0.979876</td>\n",
              "      <td>0.918367</td>\n",
              "      <td>0.981649</td>\n",
              "      <td>0.910240</td>\n",
              "      <td>1938</td>\n",
              "      <td>392</td>\n",
              "      <td>[[1899, 39], [32, 360]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.966094</td>\n",
              "      <td>0.984186</td>\n",
              "      <td>0.886836</td>\n",
              "      <td>0.974426</td>\n",
              "      <td>0.927536</td>\n",
              "      <td>0.979281</td>\n",
              "      <td>0.906730</td>\n",
              "      <td>1916</td>\n",
              "      <td>414</td>\n",
              "      <td>[[1867, 49], [30, 384]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.968240</td>\n",
              "      <td>0.982863</td>\n",
              "      <td>0.884393</td>\n",
              "      <td>0.979899</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.981379</td>\n",
              "      <td>0.892128</td>\n",
              "      <td>1990</td>\n",
              "      <td>340</td>\n",
              "      <td>[[1950, 40], [34, 306]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.969528</td>\n",
              "      <td>0.985567</td>\n",
              "      <td>0.889744</td>\n",
              "      <td>0.978005</td>\n",
              "      <td>0.925333</td>\n",
              "      <td>0.981772</td>\n",
              "      <td>0.907190</td>\n",
              "      <td>1955</td>\n",
              "      <td>375</td>\n",
              "      <td>[[1912, 43], [28, 347]]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy  precision_inanimate  ...  support_animate         confusion_matrix\n",
              "0  0.969099             0.982106  ...              372  [[1921, 37], [35, 337]]\n",
              "1  0.971674             0.980493  ...              392  [[1910, 28], [38, 354]]\n",
              "2  0.969099             0.983497  ...              383  [[1907, 40], [32, 351]]\n",
              "3  0.967382             0.984424  ...              388  [[1896, 46], [30, 358]]\n",
              "4  0.966953             0.975971  ...              391  [[1909, 30], [47, 344]]\n",
              "5  0.965665             0.976911  ...              391  [[1904, 35], [45, 346]]\n",
              "6  0.969528             0.983428  ...              392  [[1899, 39], [32, 360]]\n",
              "7  0.966094             0.984186  ...              414  [[1867, 49], [30, 384]]\n",
              "8  0.968240             0.982863  ...              340  [[1950, 40], [34, 306]]\n",
              "9  0.969528             0.985567  ...              375  [[1912, 43], [28, 347]]\n",
              "\n",
              "[10 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5Tam1n3B4wn"
      },
      "source": [
        "**Means of the Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIkYCbzDgR97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "88fc0ac9-840f-4192-a13f-471b5991dcb3"
      },
      "source": [
        "if retrain_animacy_classifier == True:\n",
        "\n",
        "  data = {'accuracy': np.mean(accuracy_list), 'precision_inanimate': np.mean(precision_list_0), 'precision_animate': np.mean(precision_list_1), 'recall_inanimate': np.mean(recall_list_0), 'recall_animate': np.mean(recall_list_1), 'f1-score_inanimate': np.mean(fscore_list_0), 'f1-score_animate': np.mean(fscore_list_1), 'support_inanimate': np.mean(support_list_0), 'support_animate': np.mean(support_list_1), 'confusion_matrix': str(np.mean(np.array(CM_list), axis=0))}\n",
        "  df_evaluation_mean = pd.DataFrame(data, index=[0])\n",
        "  \n",
        "  filename = working_dir + 'data/animacy_detection/output/' + 'AnimacyDetection_SVM_evaluation_mean' + str(n_fold_crossvalidation) + 'foldCV.pickle'\n",
        "  df_evaluation_mean.to_pickle(filename)\n",
        "  \n",
        "  df_evaluation_mean"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_inanimate</th>\n",
              "      <th>precision_animate</th>\n",
              "      <th>recall_inanimate</th>\n",
              "      <th>recall_animate</th>\n",
              "      <th>f1-score_inanimate</th>\n",
              "      <th>f1-score_animate</th>\n",
              "      <th>support_inanimate</th>\n",
              "      <th>support_animate</th>\n",
              "      <th>confusion_matrix</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.968326</td>\n",
              "      <td>0.981945</td>\n",
              "      <td>0.900276</td>\n",
              "      <td>0.980111</td>\n",
              "      <td>0.908405</td>\n",
              "      <td>0.981018</td>\n",
              "      <td>0.904124</td>\n",
              "      <td>1946.2</td>\n",
              "      <td>383.8</td>\n",
              "      <td>[[1907.5   38.7]\\n [  35.1  348.7]]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy  ...                     confusion_matrix\n",
              "0  0.968326  ...  [[1907.5   38.7]\\n [  35.1  348.7]]\n",
              "\n",
              "[1 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezxeRznIUEbV"
      },
      "source": [
        "### Save Model to File\n",
        "Finally, the classifier is trained on the entire data without leaving out a validation data set and is saved to `models/AnimacyDetection_SVM_model.pickle`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp4C5TdtUJfT",
        "outputId": "e517b2bf-5464-44a4-bb70-eecafa580527"
      },
      "source": [
        "if retrain_animacy_classifier == True:\n",
        "\n",
        "  filename = working_dir + 'data/animacy_detection/output/AnimacyDetection_SVM_input.pickle' # load preprocessed input file\n",
        "  df = pd.read_pickle(filename)\n",
        "\n",
        "  # save pos_tags list to be able to match it with new data to classify\n",
        "  df_column_names = df.columns\n",
        "  pos_dummy_set = [x for x in df_column_names if (x.startswith('we_') == False and x.startswith('ne_') == False and x != 'animacy')]\n",
        "  pos_tags_file = working_dir + 'data/animacy_detection/output/AnimacyDetection_SVM_pos-tags.pickle'\n",
        "  with open(pos_tags_file, \"wb\") as fp:\n",
        "    pickle.dump(pos_dummy_set, fp)\n",
        "\n",
        "  Y = np.array(df['animacy']) # target column\n",
        "  Y = Y.astype('int')\n",
        "  X = df.drop(['animacy'], axis=1) # input features \n",
        "\n",
        "  # training\n",
        "  svclassifier = SVC(kernel='linear')\n",
        "  svclassifier.fit(X, Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9hvGW3XUJfX"
      },
      "source": [
        "if retrain_animacy_classifier == True:\n",
        "  svm_model_file = working_dir + 'models/AnimacyDetection_SVM_model.pickle'\n",
        "  pickle.dump(svclassifier, open(svm_model_file, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0A89kwYjyFP"
      },
      "source": [
        "### Measure Feature Contribution\n",
        "In the following section the feature contributions are calculated. Due to the necessity to generate dummy variables for the pos tags and embeddings, the contribution of the single dummy variables need to be summed up to calculate the actual feature impact of word embeddings, neighborhood embeddings and pos tags for the final classification of animacy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_xw0BdnkN2z"
      },
      "source": [
        "if retrain_animacy_classifier == True:\n",
        "  feature_input_list = df.columns\n",
        "  train_column_names = list(feature_input_list)\n",
        "  train_column_names.remove('animacy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Jzowg_s0bQh"
      },
      "source": [
        "if retrain_animacy_classifier == True:\n",
        "  pos_column_names = []\n",
        "  for name in train_column_names:\n",
        "    if name.startswith('we_'):\n",
        "      pass\n",
        "    elif name.startswith('ne_'):\n",
        "      pass\n",
        "    elif name == 'semrols':\n",
        "      pass\n",
        "    elif name in ['pos', 'animacy']:\n",
        "      pass\n",
        "    else: \n",
        "      pos_column_names.append(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKKtsB5SmiFR"
      },
      "source": [
        "# function to measure the contribution of input features\n",
        "def f_importances(coef, names, semroles_included):\n",
        "    imp = np.abs(coef[0])\n",
        "    imp,names = zip(*sorted(zip(imp,names)))\n",
        "\n",
        "    # initilizations\n",
        "    we_contribution = 0\n",
        "    ne_contribution = 0\n",
        "    pos_contribution = 0\n",
        "    semroles_contribution = 0\n",
        "\n",
        "    # sum up the contribution of the single dummy variables\n",
        "    for index, name in enumerate(names):\n",
        "      if name.startswith('we_'):\n",
        "        we_contribution += imp[index]\n",
        "      elif name.startswith('ne_'):\n",
        "        ne_contribution += imp[index]\n",
        "      elif name in pos_column_names: \n",
        "        pos_contribution += imp[index]\n",
        "      else:\n",
        "        semroles_contribution += imp[index]\n",
        "    \n",
        "    if semroles_included == True:\n",
        "      names_summarized = ['word embeddings', 'neighborhood embeddings', 'pos tags', 'semantic roles']\n",
        "      imp_tmp = [we_contribution, ne_contribution, pos_contribution, semroles_contribution]\n",
        "      total = sum(imp_tmp)\n",
        "      imp_summarized = [x/total for x in imp_tmp]\n",
        "    else:\n",
        "      names_summarized = ['word embeddings', 'neighborhood embeddings', 'pos tags']\n",
        "      imp_tmp = [we_contribution, ne_contribution, pos_contribution]\n",
        "      total = sum(imp_tmp)\n",
        "      imp_summarized = [x/total for x in imp_tmp]\n",
        "\n",
        "    imp_summarized,names_summarized = zip(*sorted(zip(imp_summarized,names_summarized)))\n",
        "\n",
        "    plt.barh(range(len(names_summarized)), imp_summarized, align='center')\n",
        "    plt.yticks(range(len(names_summarized)), names_summarized)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "BCTojAIumiMT",
        "outputId": "63a7c0d9-826d-4455-abb1-986ac085441a"
      },
      "source": [
        "if retrain_animacy_classifier == True:\n",
        "  f_importances(svclassifier.coef_, train_column_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saPBf95bqyeX"
      },
      "source": [
        "del df # release memory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCxzt0ElgSIr"
      },
      "source": [
        "## Predict Animacy for new Data\n",
        "\n",
        "Now, the animacy classifier is applied to a new test. We exemplarily selected the fairytale \"Catskin\" by Joseph Jacobs as unseen text for the trained classifier. Since the new text need to be in the same format as the training data of the classifier, some preprocessing steps are required in order to predict the animate entities in the new text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibe-qnLegSLg"
      },
      "source": [
        "file_path = working_dir + 'test_data_animacy/Catskin_JosephJacobs.txt'\n",
        "\n",
        "with open(file_path, \"r\") as txt_file: # read input text file\n",
        "  text = txt_file.read()"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7HSFF5hW-pq"
      },
      "source": [
        "### Generate Tokens and POS-tags\n",
        "\n",
        "The Stanza library provides the official Python wrapper for accessing the Java Stanford CoreNLP package that was originally used to perform the tokenization and generate the part-of-speech-tags for the Russian Fairytales data set. Therefore, we have to download the English model und apply the respective functions to the new text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNxkr71OG30y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "053f54e7-2a06-429b-8378-c417601e07c5"
      },
      "source": [
        "stanza.download('en') # download English model\n",
        "nlp = stanza.Pipeline('en') # initialize English neural pipeline"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 13.2MB/s]                    \n",
            "2021-03-31 08:56:55 INFO: Downloading default packages for language: en (English)...\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.2.0/en/default.zip: 100%|██████████| 411M/411M [01:20<00:00, 5.09MB/s]\n",
            "2021-03-31 08:58:23 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
            "2021-03-31 08:58:23 INFO: Loading these models for language: en (English):\n",
            "=========================\n",
            "| Processor | Package   |\n",
            "-------------------------\n",
            "| tokenize  | combined  |\n",
            "| pos       | combined  |\n",
            "| lemma     | combined  |\n",
            "| depparse  | combined  |\n",
            "| sentiment | sstplus   |\n",
            "| ner       | ontonotes |\n",
            "=========================\n",
            "\n",
            "2021-03-31 08:58:23 INFO: Use device: gpu\n",
            "2021-03-31 08:58:23 INFO: Loading: tokenize\n",
            "2021-03-31 08:58:32 INFO: Loading: pos\n",
            "2021-03-31 08:58:32 INFO: Loading: lemma\n",
            "2021-03-31 08:58:32 INFO: Loading: depparse\n",
            "2021-03-31 08:58:33 INFO: Loading: sentiment\n",
            "2021-03-31 08:58:33 INFO: Loading: ner\n",
            "2021-03-31 08:58:34 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYK31KUjG34l"
      },
      "source": [
        "doc = nlp(text) # run annotation over the text"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOnby6elOVTD"
      },
      "source": [
        "tokens = []\n",
        "pos_tags = []\n",
        "\n",
        "for entry in doc.iter_tokens(): # iterate over tokens of input text\n",
        "  dict_entry = entry.to_dict()\n",
        "  tokens.append(dict_entry[0]['text']) # add token to list \n",
        "  pos_tags.append(dict_entry[0]['xpos']) # add pos-tag to list"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcYdayQVG4D8"
      },
      "source": [
        "data = {'token': tokens, 'pos': pos_tags} # setup data for the DataFrame\n",
        "df_new_text = pd.DataFrame(data, columns=['token', 'pos']) # create DataFrame"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsPyX811YmXo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "89aa053d-c1c9-4e2b-d8e4-1a7ca77d60e4"
      },
      "source": [
        "df_new_text"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Well</td>\n",
              "      <td>UH</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>there</td>\n",
              "      <td>EX</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>was</td>\n",
              "      <td>VBD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>once</td>\n",
              "      <td>RB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1918</th>\n",
              "      <td>lived</td>\n",
              "      <td>VBD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1919</th>\n",
              "      <td>happy</td>\n",
              "      <td>JJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1920</th>\n",
              "      <td>ever</td>\n",
              "      <td>RB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1921</th>\n",
              "      <td>afterwards</td>\n",
              "      <td>RB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1922</th>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1923 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           token  pos\n",
              "0           Well   UH\n",
              "1              ,    ,\n",
              "2          there   EX\n",
              "3            was  VBD\n",
              "4           once   RB\n",
              "...          ...  ...\n",
              "1918       lived  VBD\n",
              "1919       happy   JJ\n",
              "1920        ever   RB\n",
              "1921  afterwards   RB\n",
              "1922           .    .\n",
              "\n",
              "[1923 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x0mxsTGG4LX"
      },
      "source": [
        "### Compute Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRDL_FePUnkv"
      },
      "source": [
        "**Run Bert Client (if not already running)**\n",
        "\n",
        "Download the Bert Base Case model if not already present under `data/bert_model/cased_L-12_H-768_A-12` und start the BertClient if it is not already running.  \n",
        "\n",
        "The stdout and stderr of the nohup process which is starting the BertClient() are forwarded to the file data/bert_model/log/bert-serving-start_output.log. The server is finally initialized when the last line of the logfile displays: \"all set, ready to serve request!\". The initialization requires about a minute. The \"!cat logfile\" cell can be reexecuted to check the initialization status of the Bert client."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "KFPWL3sjMIRr"
      },
      "source": [
        "download_bert_model = False #@param [\"True\", \"False\"] {type:\"raw\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "E6dcgVf6KZuk"
      },
      "source": [
        "client_already_running = False #@param [\"True\", \"False\"] {type:\"raw\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMsiSbu-KNbE"
      },
      "source": [
        "if download_bert_model == True:\n",
        "  bert_model_download_destination = working_dir_extern + 'data/bert_model/'\n",
        "  !wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip -P {bert_model_download_destination}\n",
        "  downloaded_bert_model_zip = bert_model_download_destination + 'cased_L-12_H-768_A-12.zip'\n",
        "  !unzip {downloaded_bert_model_zip} -d {bert_model_download_destination}\n",
        "  download_bert_model = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAClOig3SMtp"
      },
      "source": [
        "if client_already_running == False:\n",
        "  log_file = working_dir_extern + 'data/bert_model/log/bert-serving-start_output.log'\n",
        "  bert_model_filepath = working_dir_extern + 'data/bert_model/cased_L-12_H-768_A-12/'\n",
        "  !nohup bert-serving-start -model_dir {bert_model_filepath} -pooling_strategy NONE -max_seq_len 145 -num_worker=4 > {log_file} 2>&1 &"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXkfnY89SMtr"
      },
      "source": [
        "if client_already_running == False:\n",
        "  # check the nohup output file to see if the bert server client is running correctly\n",
        "  log_file = working_dir_extern + 'data/bert_model/log/bert-serving-start_output.log'\n",
        "  !cat {log_file}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ipe1XXVUVva"
      },
      "source": [
        "if client_already_running == False:\n",
        "  client = BertClient() # initialize Bert client"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kDEOFm4Vn1V"
      },
      "source": [
        "**Compute Word Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwjEIH_8Vn1f"
      },
      "source": [
        "pbar = ProgressBar() # initialize progress bar\n",
        "embedding_list = []\n",
        "\n",
        "for index, row in pbar(df_new_text.iterrows()): # iterate over rows of the new df\n",
        "  token = row['token']\n",
        "  embedding = client.encode([token])[0][1] # compute Bert embedding for each single token\n",
        "  embedding_list.append(embedding)\n",
        "\n",
        "df_new_text['word_embedding'] = embedding_list # insert a new column with the word embeddings to the df\n",
        "df_new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AsjzMBAVn1k"
      },
      "source": [
        "**Compute Neighborhood Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDjk_sOvVn1l"
      },
      "source": [
        "pbar = ProgressBar() # initialize progress bar\n",
        "neighborhood_threshhold = 3\n",
        "\n",
        "nrows = len(df_new_text.index)\n",
        "neighborhood_embedding_list = []\n",
        "  \n",
        "for index, row in pbar(df_new_text.iterrows()): # iterate over rows of the df\n",
        "  neighborhood_embedding = [0]*768 # initialize neighborhood embeddings array with 768 dimensions (size of Bert embedding)\n",
        "  if index == 0: # special case for the first word of a text\n",
        "    for i in range(1, neighborhood_threshhold+1):\n",
        "      we_index = index + i\n",
        "      vec = df_new_text.at[we_index, 'word_embedding']\n",
        "      neighborhood_embedding = neighborhood_embedding + vec\n",
        "  elif index == 1: # special case for the second word of a text\n",
        "    for i in range(-1, neighborhood_threshhold+1):\n",
        "      if i != 0: # exclude the current embedding\n",
        "        we_index = index + i\n",
        "        vec = df_new_text.at[we_index, 'word_embedding']\n",
        "        neighborhood_embedding = neighborhood_embedding + vec\n",
        "  elif index == 2: # special case for the third word of a text\n",
        "    for i in range(-2, neighborhood_threshhold+1):\n",
        "      if i != 0: # exclude the current embedding\n",
        "        we_index = index + i\n",
        "        vec = df_new_text.at[we_index, 'word_embedding']\n",
        "        neighborhood_embedding = neighborhood_embedding + vec\n",
        "  elif index == nrows-3: # special case for the third last word of a text\n",
        "    for i in range(-3, neighborhood_threshhold):\n",
        "      if i != 0: # exclude the current embedding\n",
        "        we_index = index + i\n",
        "        vec = df_new_text.at[we_index, 'word_embedding']\n",
        "        neighborhood_embedding = neighborhood_embedding + vec\n",
        "  elif index == nrows-2: # special case for the next-to-last word of a text\n",
        "    for i in range(-3, neighborhood_threshhold-1):\n",
        "      if i != 0: # exclude the current embedding\n",
        "        we_index = index + i\n",
        "        vec = df_new_text.at[we_index, 'word_embedding']\n",
        "        neighborhood_embedding = neighborhood_embedding + vec\n",
        "  elif index == nrows-1: # special case for the last word of a text\n",
        "    for i in range(-3, neighborhood_threshhold-2):\n",
        "      if i != 0: # exclude the current embedding\n",
        "        we_index = index + i\n",
        "        vec = df_new_text.at[we_index, 'word_embedding']\n",
        "        neighborhood_embedding = neighborhood_embedding + vec\n",
        "  else: # normal case (current token has three other token before and after it)\n",
        "    for i in range(-3, neighborhood_threshhold+1):\n",
        "      if i != 0: # exclude the current embedding\n",
        "        we_index = index + i\n",
        "        vec = df_new_text.at[we_index, 'word_embedding']\n",
        "        neighborhood_embedding = neighborhood_embedding + vec    \n",
        "    \n",
        "  neighborhood_embedding_list.append(neighborhood_embedding)\n",
        "\n",
        "df_new_text['neighborhood_embedding'] = neighborhood_embedding_list # insert a new column with the neighborhood embeddings to the df\n",
        "df_new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gceqwvCKOGXk"
      },
      "source": [
        "### Create Dummy Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCVb-ClDG4We"
      },
      "source": [
        "embedding_size = 768 # specify the embedding size\n",
        "\n",
        "df_pos_dummy = pd.get_dummies(df_new_text['pos']) # dummy variables for the pos tagger column\n",
        "df_new_text = pd.concat([df_new_text, df_pos_dummy], axis=1) # concatenate the new columns for the dummy variables to the df\n",
        "df_new_text_column_names = df_new_text.columns\n",
        "pos_tags_file = working_dir + 'data/animacy_detection/output/AnimacyDetection_SVM_pos-tags.pickle'\n",
        "with open(pos_tags_file, \"rb\") as fp: #load set of dummy variables from training data \n",
        "  pos_dummy_set = pickle.load(fp)\n",
        "null_vector = [0] * df_new_text.shape[0]\n",
        "for variable in pos_dummy_set: #insert null vectors for all missing dummy variables compared to the training data \n",
        "  if variable not in df_new_text_column_names:\n",
        "    df_new_text[variable] = 0\n",
        "\n",
        "\n",
        "# dummy variables for embedding columns (word embedding and neighborhood embedding)\n",
        "we_dummy = []\n",
        "ne_dummy = []\n",
        "for index, row in df_new_text.iterrows(): # iterate over rows of the df containing ALL tokens of the training data\n",
        "  we = row['word_embedding']\n",
        "  ne = row['neighborhood_embedding']\n",
        "  row_vector_we = []\n",
        "  row_vector_ne = []\n",
        "  for count, val in enumerate(we): # iterate over word embedding\n",
        "    row_vector_we.append(val)\n",
        "  for count, val in enumerate(ne): # iterate over neighborhood embedding\n",
        "    row_vector_ne.append(val)\n",
        "  we_dummy.append(row_vector_we)\n",
        "  ne_dummy.append(row_vector_ne)\n",
        "\n",
        "# create names for the embedding dummy variables\n",
        "we_dummy_names = []\n",
        "ne_dummy_names = []\n",
        "for i in range(0, embedding_size): \n",
        "  we_dummy_names.append('we_' + str(i))\n",
        "  ne_dummy_names.append('ne_' + str(i))\n",
        "\n",
        "df_we_dummy = pd.DataFrame(data=we_dummy, columns=we_dummy_names) # create df for word embedding dummy variables\n",
        "df_ne_dummy = pd.DataFrame(data=ne_dummy, columns=ne_dummy_names) # create df for neighborhood embedding dummy variables\n",
        "\n",
        "df_new_text = pd.concat([df_new_text, df_we_dummy, df_ne_dummy], axis=1) # concatenate the new columns for the dummy variables to the original df\n",
        "df_tokens_only = df_new_text[['token']]\n",
        "df_new_text = df_new_text.drop(['token', 'pos', 'word_embedding', 'neighborhood_embedding'], axis=1) # drop all columns that are not used in the SVM classification\n",
        "\n",
        "df_new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEL_YtrBOCEI"
      },
      "source": [
        "### Load Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo7UxZcRTYY3",
        "outputId": "b49a1501-3185-4ace-dd0b-690887342cba"
      },
      "source": [
        "svm_model_file = working_dir + 'models/AnimacyDetection_SVM_model.pickle'\n",
        "# svm_model_file = working_dir + 'data/jahan_animacy_corpus/pickles/AnimacyDetection_SVM_model.pickle'\n",
        "svclassifier = pickle.load(open(svm_model_file, 'rb'))\n",
        "svclassifier"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxBCugnnSxcR"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP0sJO-Uwzzs"
      },
      "source": [
        "df_new_text = df_new_text.reindex(columns=df_column_names)\n",
        "df_new_text = df_new_text.drop(columns=['animacy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3A6nIIGOCQS"
      },
      "source": [
        "Y_pred = svclassifier.predict(df_new_text) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NudRSVmKOCUA"
      },
      "source": [
        "print(Y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqVHZgxqzg4t"
      },
      "source": [
        "df_tokens_only"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CuEDd2V1tiL"
      },
      "source": [
        "df_tokens_only['animacy'] = Y_pred\n",
        "df_tokens_only.head(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWlGI76ArPLs"
      },
      "source": [
        "## Include SRL to Animacy Detection\n",
        "In this step a fundamental proplem regarding the addition of semantic role labeling as in input feature to the animacy detection becomes aparent. Semantic roles are annotated for each predicate in a sentence and for different predicates in the same sentence the argument annotation spans can overlap as it can be seen in the example below. If one would create a training instance of the same sentence with SRL annotation for each predicate in the sentence, it would cause a clear overfit. \n",
        "\n",
        "Nontheless, we wanted to investigate if any model improvement can be achieved by including the SRLs and therefore we decided to annotate semantic roles for each senetence once. To overcome the overlapping spans conflict, we implemented a concept such that a new semantic role label can only be assigned to tokens that do not already have another SRL label. Thus, it is not a perfect SRL annotation, but we were able to analyze a possible performance impact of the animacy classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPewm6t7rPQK"
      },
      "source": [
        "![multiple_frames.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_3LZgR-5FFX"
      },
      "source": [
        "To determine whether the evaluation of the classifier with additional semantic role labels should be performed (again), set the variable `repeat_evaluation`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "MOV-ND8e5FFZ"
      },
      "source": [
        "repeat_evaluation = False #@param [\"True\", \"False\"] {type:\"raw\"}"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCiz4CGFrPT_",
        "outputId": "0d15e206-64ae-43ac-8723-92950cacade8"
      },
      "source": [
        "if repeat_evaluation == True:\n",
        "\n",
        "  ### Get SRL annotations from parsed files of single stories\n",
        "  \n",
        "  pbar = ProgressBar() # initialize a progress bar\n",
        "  semroles_array = []\n",
        "  \n",
        "  for filename in pbar(input_files): # iterate over input files\n",
        "    input_filename = working_dir + 'data/russian_fairytales/parsed_pickles' + filename[:-4] + '_df.pickle'\n",
        "    df = pd.read_pickle(input_filename)\n",
        "    semroles_story = df['semrols']\n",
        "    semroles_array.extend(semroles_story)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% (15 of 15) |########################| Elapsed Time: 0:00:10 Time:  0:00:10\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KknYDUyI3dca"
      },
      "source": [
        "Unfortunately, the entire preprocessed training data set exceeds the maximum file size allowed by github. If one want to repeat the evaluation of the SVM classifier with semantic roles, the required file can be manually downloaded by clicking on the following link: \n",
        "https://drive.google.com/u/0/uc?export=download&confirm=S9Jm&id=1-CpTgM7WfSPdpNnEFEignMgWheFwrbue\n",
        "\n",
        "Since Google implemented an extra information that large files can not be scanned for viruses, the download can not be automated and performed by wget. The file need to be saved under `data/animacy_detection/output/`. After the download is completed, one can continue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E44897lBnBZC"
      },
      "source": [
        "if repeat_evaluation == True:\n",
        "  \n",
        "  # load downloaded input training data to pandas data frame\n",
        "  filename = working_dir + 'data/animacy_detection/output/AnimacyDetection_SVM_input.pickle' # load preprocessed input file\n",
        "  df = pd.read_pickle(filename)\n",
        "  all_column_names = df.columns"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H9TZNekrPaw"
      },
      "source": [
        "if repeat_evaluation == True:\n",
        "\n",
        "  # add the semantic role annotations to the training data frame\n",
        "  df['semrols'] = semroles_array\n",
        "  df_column_names = df.columns"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqv7WPmT1PiZ"
      },
      "source": [
        "if repeat_evaluation == True:\n",
        "\n",
        "  # create dummy variables for the semantic roles\n",
        "  df_semrols_dummy = pd.get_dummies(df['semrols']) # dummy variables\n",
        "  df = pd.concat([df, df_semrols_dummy], axis=1) # concatenate the new columns for the dummy variables to the df\n",
        "  df = df.drop(['semrols'], axis=1)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouQZlnT-rPeM",
        "outputId": "eac524a1-7c3a-40fc-c515-4bb348f15e4e"
      },
      "source": [
        "if repeat_evaluation == True:\n",
        "  \n",
        "  # separate target column \n",
        "  Y = Y.astype('int')\n",
        "  X = df.drop(['animacy'], axis=1) # input features \n",
        "\n",
        "  # train classifier\n",
        "  svclassifier = SVC(kernel='linear')\n",
        "  svclassifier.fit(X, Y)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqb4q1Ql8ooR"
      },
      "source": [
        "### Evaluate Model\n",
        "\n",
        "The new model is evaluated using 5-fold crossvalidation and a 90:10 train-test split. The evaluation results are saved to file and can be found in the `data/animacy_detecton/output/` directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ujmTUfHWyce",
        "outputId": "3bff9b8c-81d4-433d-e578-b7b6afa4b13e"
      },
      "source": [
        "# initialization for the classifier evaluation\n",
        "accuracy_list = []\n",
        "precision_list_0 = []\n",
        "precision_list_1 = []\n",
        "recall_list_0 = []\n",
        "recall_list_1 = []\n",
        "fscore_list_0 = []\n",
        "fscore_list_1 = []\n",
        "support_list_0 = []\n",
        "support_list_1 = []\n",
        "CM_list = []\n",
        "\n",
        "pbar = ProgressBar() # initialize a progress bar\n",
        "\n",
        "n_fold_crossvalidation = 5 # number of cross-validations\n",
        "\n",
        "for i in pbar(range(0, n_fold_crossvalidation)):\n",
        "\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1) # split data into 90% training set and 10% test set\n",
        "  \n",
        "  # training\n",
        "  svclassifier = SVC(kernel='linear')\n",
        "  svclassifier.fit(X_train, Y_train)\n",
        "\n",
        "  # prediction\n",
        "  Y_pred = svclassifier.predict(X_test) \n",
        "\n",
        "  # elaluation\n",
        "  CM = confusion_matrix(Y_test, Y_pred)\n",
        "  CM_list.append(CM)\n",
        "\n",
        "  acc = accuracy_score(Y_test, Y_pred)\n",
        "  accuracy_list.append(acc)\n",
        "\n",
        "  precision,recall,fscore,support=score(Y_test, Y_pred)\n",
        "  precision_list_0.append(precision[0])\n",
        "  recall_list_0.append(recall[0])\n",
        "  fscore_list_0.append(fscore[0])\n",
        "  support_list_0.append(support[0])\n",
        "  precision_list_1.append(precision[1])\n",
        "  recall_list_1.append(recall[1])\n",
        "  fscore_list_1.append(fscore[1])\n",
        "  support_list_1.append(support[1])"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% (5 of 5) |##########################| Elapsed Time: 0:16:34 Time:  0:16:34\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "sqa2GRodXw25",
        "outputId": "3c16c23a-b12b-465e-e344-b0c05a17a0e8"
      },
      "source": [
        "if repeat_evaluation == False:\n",
        "\n",
        "  data = {'accuracy': accuracy_list, 'precision_inanimate': precision_list_0, 'precision_animate': precision_list_1, 'recall_inanimate': recall_list_0, 'recall_animate': recall_list_1, 'f1-score_inanimate': fscore_list_0, 'f1-score_animate': fscore_list_1, 'support_inanimate': support_list_0, 'support_animate': support_list_1, 'confusion_matrix': CM_list}\n",
        "  df_evaluation = pd.DataFrame(data, columns=['accuracy', 'precision_inanimate', 'precision_animate', 'recall_inanimate', 'recall_animate', 'f1-score_inanimate', 'f1-score_animate', 'support_inanimate', 'support_animate', 'confusion_matrix'])\n",
        "\n",
        "  filename = working_dir + 'data/animacy_detection/output/' + 'AnimacyDetection_SVM_WithSRL_evaluation_' + str(n_fold_crossvalidation) + 'foldCV.pickle'\n",
        "  df_evaluation.to_pickle(filename)\n",
        "\n",
        "df_evaluation = pd.read_pickle(working_dir + 'data/animacy_detection/output/' + 'AnimacyDetection_SVM_WithSRL_evaluation_' + str(n_fold_crossvalidation) + 'foldCV.pickle')\n",
        "df_evaluation"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_inanimate</th>\n",
              "      <th>precision_animate</th>\n",
              "      <th>recall_inanimate</th>\n",
              "      <th>recall_animate</th>\n",
              "      <th>f1-score_inanimate</th>\n",
              "      <th>f1-score_animate</th>\n",
              "      <th>support_inanimate</th>\n",
              "      <th>support_animate</th>\n",
              "      <th>confusion_matrix</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.969957</td>\n",
              "      <td>0.979285</td>\n",
              "      <td>0.924812</td>\n",
              "      <td>0.984383</td>\n",
              "      <td>0.902200</td>\n",
              "      <td>0.981828</td>\n",
              "      <td>0.913366</td>\n",
              "      <td>1921</td>\n",
              "      <td>409</td>\n",
              "      <td>[[1891, 30], [40, 369]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.963948</td>\n",
              "      <td>0.977023</td>\n",
              "      <td>0.903614</td>\n",
              "      <td>0.979069</td>\n",
              "      <td>0.894988</td>\n",
              "      <td>0.978045</td>\n",
              "      <td>0.899281</td>\n",
              "      <td>1911</td>\n",
              "      <td>419</td>\n",
              "      <td>[[1871, 40], [44, 375]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.965236</td>\n",
              "      <td>0.976004</td>\n",
              "      <td>0.915254</td>\n",
              "      <td>0.981637</td>\n",
              "      <td>0.891509</td>\n",
              "      <td>0.978812</td>\n",
              "      <td>0.903226</td>\n",
              "      <td>1906</td>\n",
              "      <td>424</td>\n",
              "      <td>[[1871, 35], [46, 378]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.968670</td>\n",
              "      <td>0.978888</td>\n",
              "      <td>0.917526</td>\n",
              "      <td>0.983445</td>\n",
              "      <td>0.896725</td>\n",
              "      <td>0.981161</td>\n",
              "      <td>0.907006</td>\n",
              "      <td>1933</td>\n",
              "      <td>397</td>\n",
              "      <td>[[1901, 32], [41, 356]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.972532</td>\n",
              "      <td>0.988735</td>\n",
              "      <td>0.888594</td>\n",
              "      <td>0.978713</td>\n",
              "      <td>0.938375</td>\n",
              "      <td>0.983698</td>\n",
              "      <td>0.912807</td>\n",
              "      <td>1973</td>\n",
              "      <td>357</td>\n",
              "      <td>[[1931, 42], [22, 335]]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy  precision_inanimate  precision_animate  recall_inanimate  \\\n",
              "0  0.969957             0.979285           0.924812          0.984383   \n",
              "1  0.963948             0.977023           0.903614          0.979069   \n",
              "2  0.965236             0.976004           0.915254          0.981637   \n",
              "3  0.968670             0.978888           0.917526          0.983445   \n",
              "4  0.972532             0.988735           0.888594          0.978713   \n",
              "\n",
              "   recall_animate  f1-score_inanimate  f1-score_animate  support_inanimate  \\\n",
              "0        0.902200            0.981828          0.913366               1921   \n",
              "1        0.894988            0.978045          0.899281               1911   \n",
              "2        0.891509            0.978812          0.903226               1906   \n",
              "3        0.896725            0.981161          0.907006               1933   \n",
              "4        0.938375            0.983698          0.912807               1973   \n",
              "\n",
              "   support_animate         confusion_matrix  \n",
              "0              409  [[1891, 30], [40, 369]]  \n",
              "1              419  [[1871, 40], [44, 375]]  \n",
              "2              424  [[1871, 35], [46, 378]]  \n",
              "3              397  [[1901, 32], [41, 356]]  \n",
              "4              357  [[1931, 42], [22, 335]]  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "pjfdOCNBXw27",
        "outputId": "1c8f5763-1bff-4bd6-a260-381967c0ed93"
      },
      "source": [
        "if repeat_evaluation == False:\n",
        "\n",
        "  data = {'accuracy': np.mean(accuracy_list), 'precision_inanimate': np.mean(precision_list_0), 'precision_animate': np.mean(precision_list_1), 'recall_inanimate': np.mean(recall_list_0), 'recall_animate': np.mean(recall_list_1), 'f1-score_inanimate': np.mean(fscore_list_0), 'f1-score_animate': np.mean(fscore_list_1), 'support_inanimate': np.mean(support_list_0), 'support_animate': np.mean(support_list_1), 'confusion_matrix': str(np.mean(np.array(CM_list), axis=0))}\n",
        "  df_evaluation_mean = pd.DataFrame(data, index=[0])\n",
        "\n",
        "  #filename = working_dir + 'data/animacy_detection/output/' + 'AnimacyDetection_SVM_WithSRL_evaluation_mean' + str(n_fold_crossvalidation) + 'foldCV.pickle'\n",
        "  filename = working_dir + 'data/jahan_animacy_corpus/pickles/' + 'AnimacyDetection_SVM_WithSRL_evaluation_mean' + str(n_fold_crossvalidation) + 'foldCV.pickle'\n",
        "  df_evaluation_mean.to_pickle(filename)\n",
        "\n",
        "df_evaluation_mean = pd.read_pickle(working_dir + 'data/jahan_animacy_corpus/pickles/' + 'AnimacyDetection_SVM_WithSRL_evaluation_mean' + str(n_fold_crossvalidation) + 'foldCV.pickle')\n",
        "df_evaluation_mean"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_inanimate</th>\n",
              "      <th>precision_animate</th>\n",
              "      <th>recall_inanimate</th>\n",
              "      <th>recall_animate</th>\n",
              "      <th>f1-score_inanimate</th>\n",
              "      <th>f1-score_animate</th>\n",
              "      <th>support_inanimate</th>\n",
              "      <th>support_animate</th>\n",
              "      <th>confusion_matrix</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.968069</td>\n",
              "      <td>0.979987</td>\n",
              "      <td>0.90996</td>\n",
              "      <td>0.981449</td>\n",
              "      <td>0.90476</td>\n",
              "      <td>0.980709</td>\n",
              "      <td>0.907137</td>\n",
              "      <td>1928.8</td>\n",
              "      <td>401.2</td>\n",
              "      <td>[[1893.    35.8]\\n [  38.6  362.6]]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy  precision_inanimate  precision_animate  recall_inanimate  \\\n",
              "0  0.968069             0.979987            0.90996          0.981449   \n",
              "\n",
              "   recall_animate  f1-score_inanimate  f1-score_animate  support_inanimate  \\\n",
              "0         0.90476            0.980709          0.907137             1928.8   \n",
              "\n",
              "   support_animate                     confusion_matrix  \n",
              "0            401.2  [[1893.    35.8]\\n [  38.6  362.6]]  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObH_g8Ts9GKI"
      },
      "source": [
        "### Measure Feature Contribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tW5PrIa7VwN"
      },
      "source": [
        "if repeat_evaluation == True:\n",
        "  feature_input_list = df.columns\n",
        "  train_column_names = list(feature_input_list)\n",
        "  train_column_names.remove('animacy')"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnhtUe3_7Vwb"
      },
      "source": [
        "if repeat_evaluation == True:\n",
        "  pos_column_names = []\n",
        "  for name in train_column_names:\n",
        "    if name.startswith('we_'):\n",
        "      pass\n",
        "    elif name.startswith('ne_'):\n",
        "      pass\n",
        "    elif name == 'semrols':\n",
        "      pass\n",
        "    elif name in ['pos', 'animacy']:\n",
        "      pass\n",
        "    else: \n",
        "      pos_column_names.append(name)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxG1Sqhd7qaS"
      },
      "source": [
        "# function to measure the contribution of input features\n",
        "def f_importances(coef, names, semroles_included):\n",
        "    imp = np.abs(coef[0])\n",
        "    imp,names = zip(*sorted(zip(imp,names)))\n",
        "\n",
        "    # initilizations\n",
        "    we_contribution = 0\n",
        "    ne_contribution = 0\n",
        "    pos_contribution = 0\n",
        "    semroles_contribution = 0\n",
        "\n",
        "    # sum up the contribution of the single dummy variables\n",
        "    for index, name in enumerate(names):\n",
        "      if name.startswith('we_'):\n",
        "        we_contribution += imp[index]\n",
        "      elif name.startswith('ne_'):\n",
        "        ne_contribution += imp[index]\n",
        "      elif name in pos_column_names: \n",
        "        pos_contribution += imp[index]\n",
        "      else:\n",
        "        semroles_contribution += imp[index]\n",
        "    \n",
        "    if semroles_included == True:\n",
        "      names_summarized = ['word embeddings', 'neighborhood embeddings', 'pos tags', 'semantic roles']\n",
        "      imp_tmp = [we_contribution, ne_contribution, pos_contribution, semroles_contribution]\n",
        "      total = sum(imp_tmp)\n",
        "      imp_summarized = [x/total for x in imp_tmp]\n",
        "    else:\n",
        "      names_summarized = ['word embeddings', 'neighborhood embeddings', 'pos tags']\n",
        "      imp_tmp = [we_contribution, ne_contribution, pos_contribution]\n",
        "      total = sum(imp_tmp)\n",
        "      imp_summarized = [x/total for x in imp_tmp]\n",
        "\n",
        "    imp_summarized,names_summarized = zip(*sorted(zip(imp_summarized,names_summarized)))\n",
        "\n",
        "    plt.barh(range(len(names_summarized)), imp_summarized, align='center')\n",
        "    plt.yticks(range(len(names_summarized)), names_summarized)\n",
        "    plt.show()"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "OmovZIxg8Ynm",
        "outputId": "b4ce8ce6-0436-48b5-96ca-4ac3d7c8cf4c"
      },
      "source": [
        "if repeat_evaluation == True:\n",
        "  f_importances(svclassifier.coef_, X.columns, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ptg0E3T781R"
      },
      "source": [
        "## Animacy Detection using a Multilayer Perceptron (MLP)\n",
        "Furthermore, we want to compare our SVM Classifier to an Artificial Neural Network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3vdLyiJPHa_"
      },
      "source": [
        "Again, the entire preprocessed training data set is needed, which could not be uploaded to github. If not already downloaded in the previous section, one can get the required file by clicking on the following link: \n",
        "\n",
        "https://drive.google.com/u/0/uc?export=download&confirm=S9Jm&id=1-CpTgM7WfSPdpNnEFEignMgWheFwrbue\n",
        "\n",
        "The file need to be saved under `data/animacy_detection/output/`. After the download is completed, one can continue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CbxExBbUea5"
      },
      "source": [
        "### Without SRL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ycea2nkQU0B"
      },
      "source": [
        "# load downloaded input training data to pandas data frame\n",
        "filename = working_dir + 'data/animacy_detection/output/AnimacyDetection_SVM_input.pickle' # load preprocessed input file\n",
        "df = pd.read_pickle(filename)"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwHFhxvKR1OP"
      },
      "source": [
        "Y = np.array(df['animacy']) # target column\n",
        "Y = Y.astype('int')\n",
        "X = df.drop(['animacy'], axis=1) # input features "
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXKq7nqPI9IW"
      },
      "source": [
        "# create torch tensors for the neural network\n",
        "df_array = X.to_numpy()\n",
        "x_tensor = torch.tensor(df_array)\n",
        "y_tensor = torch.tensor(Y)\n",
        "\n",
        "# save the torch tensors to file\n",
        "x_tensor_filepath = working_dir + 'data/animacy_detection/output/X_TENSOR_RussianFairytales_withoutSRL.pt'\n",
        "y_tensor_filepath = working_dir + 'data/animacy_detection/output/Y_TENSOR_RussianFairytales.pt'\n",
        "torch.save(x_tensor, x_tensor_filepath)\n",
        "torch.save(y_tensor, y_tensor_filepath)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlWZy3JmI9IX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c62df06-79a7-40e9-98e1-3febbe676449"
      },
      "source": [
        "# initialize inputs with respective data type\n",
        "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(x_tensor, y_tensor,\n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "BATCH_SIZE = 32\n",
        "tr_inputs = torch.tensor(tr_inputs,dtype=torch.float32)\n",
        "val_inputs = torch.tensor(val_inputs,dtype=torch.float32)\n",
        "tr_tags = torch.tensor(tr_tags,dtype=torch.float32)\n",
        "val_tags = torch.tensor(val_tags,dtype=torch.float32)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fWldZLI4_SF"
      },
      "source": [
        "# setup tensor data set \n",
        "train_data = TensorDataset(tr_inputs, tr_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data,sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "valid_data = TensorDataset(val_inputs,val_tags)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data,sampler=valid_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cHAaIWkI9Ib"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(1582, 300)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dout = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(300, 100)\n",
        "        self.prelu = nn.PReLU(1)\n",
        "        self.out = nn.Linear(100, 1)\n",
        "        self.out_act = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, input_):\n",
        "        a1 = self.fc1(input_)\n",
        "        h1 = self.relu1(a1)\n",
        "        dout = self.dout(h1)\n",
        "        a2 = self.fc2(dout)\n",
        "        h2 = self.prelu(a2)\n",
        "        a3 = self.out(h2)\n",
        "        y = self.out_act(a3)\n",
        "        return y\n",
        "    \n",
        "net = Net()\n",
        "opt = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
        "criterion = nn.BCELoss()"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxlFNq9hQ2D3"
      },
      "source": [
        "net.cuda();\n",
        "device = \"cuda\"\n",
        "EPOCHS = 7"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw4onzB2Lng5",
        "outputId": "b78f8751-e8ea-4aee-a8e2-cdc88184933b"
      },
      "source": [
        "net.train()\n",
        "for _ in range(EPOCHS):\n",
        "\n",
        "\n",
        "  total_loss = 0\n",
        "\n",
        "  for batch in train_dataloader:\n",
        "\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_x , b_labels = batch\n",
        "    opt.zero_grad()\n",
        "    y_hat = net(b_input_x)\n",
        "    loss = criterion(y_hat, b_labels.unsqueeze(1))\n",
        "    loss.backward()\n",
        "    opt.step()        \n",
        "    total_loss += loss.item()\n",
        "\n",
        "  avg_train_loss = total_loss / len(train_dataloader)\n",
        "  print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "  net.eval()\n",
        "  predictions,realtags = [],[]\n",
        "  for batch in valid_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_x , b_labels = batch\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions.\n",
        "        # This will return the logits rather than the loss because we have not provided labels.\n",
        "        outputs = net(b_input_x)\n",
        "\n",
        "    pred_labels = outputs.squeeze()>=0.5\n",
        "    pred_labels = pred_labels.detach().cpu().numpy()\n",
        "    real_labels = b_labels==1\n",
        "    real_labels = real_labels.detach().cpu().numpy()\n",
        "    predictions.extend(pred_labels)\n",
        "    realtags.extend(real_labels)\n",
        "  print(\"Validation Accuracy: {}\".format(accuracy_score(predictions, realtags)))"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 0.14557197157587437\n",
            "Validation Accuracy: 0.9772532188841202\n",
            "Average train loss: 0.08224382251572068\n",
            "Validation Accuracy: 0.9763948497854077\n",
            "Average train loss: 0.06973047083076059\n",
            "Validation Accuracy: 0.9798283261802575\n",
            "Average train loss: 0.06356734760310047\n",
            "Validation Accuracy: 0.9793991416309012\n",
            "Average train loss: 0.05871250876661978\n",
            "Validation Accuracy: 0.9819742489270387\n",
            "Average train loss: 0.057314505754467025\n",
            "Validation Accuracy: 0.9815450643776824\n",
            "Average train loss: 0.05565193553007791\n",
            "Validation Accuracy: 0.98068669527897\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "YYeYWw4xLnjx",
        "outputId": "bf79500b-89ba-4269-cbd1-d7d3a4ddad89"
      },
      "source": [
        "confusion_matrix = pd.DataFrame(confusion_matrix(realtags, predictions))\n",
        "confusion_matrix"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1915</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15</td>\n",
              "      <td>370</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0    1\n",
              "0  1915   30\n",
              "1    15  370"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuxnYuDzLnm2"
      },
      "source": [
        "### With SRL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILZ1Usi5VkA8",
        "outputId": "0d15e206-64ae-43ac-8723-92950cacade8"
      },
      "source": [
        "### Get SRL annotations from parsed files of single stories\n",
        "\n",
        "pbar = ProgressBar() # initialize a progress bar\n",
        "semroles_array = []\n",
        "\n",
        "for filename in pbar(input_files): # iterate over input files\n",
        "  input_filename = working_dir + 'data/russian_fairytales/parsed_pickles' + filename[:-4] + '_df.pickle'\n",
        "  df = pd.read_pickle(input_filename)\n",
        "  semroles_story = df['semrols']\n",
        "  semroles_array.extend(semroles_story)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% (15 of 15) |########################| Elapsed Time: 0:00:10 Time:  0:00:10\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4OQhvY7U3KZ"
      },
      "source": [
        "# load downloaded input training data to pandas data frame\n",
        "filename = working_dir + 'data/animacy_detection/output/AnimacyDetection_SVM_input.pickle' # load preprocessed input file\n",
        "df = pd.read_pickle(filename)\n",
        "df['semrols'] = semroles_array"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzQcbdCmWMbJ"
      },
      "source": [
        "df_semrols_dummy = pd.get_dummies(df['semrols']) # dummy variables for the semrolscolumn\n",
        "df = pd.concat([df, df_semrols_dummy], axis=1) # concatenate the new columns for the dummy variables to the df\n",
        "df = df.drop(['semrols'], axis=1)"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sB2o3BUU3Kc"
      },
      "source": [
        "Y = np.array(df['animacy']) # target column\n",
        "Y = Y.astype('int')\n",
        "X = df.drop(['animacy'], axis=1) # input features "
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zul8F2yHU3Kd"
      },
      "source": [
        "# create torch tensors for the neural network\n",
        "df_array = X.to_numpy()\n",
        "x_tensor = torch.tensor(df_array)\n",
        "y_tensor = torch.tensor(Y)\n",
        "\n",
        "# save the torch tensors to file\n",
        "x_tensor_filepath = working_dir + 'data/animacy_detection/output/X_TENSOR_RussianFairytales_withSRL.pt'\n",
        "y_tensor_filepath = working_dir + 'data/animacy_detection/output/Y_TENSOR_RussianFairytales.pt'\n",
        "torch.save(x_tensor, x_tensor_filepath)\n",
        "torch.save(y_tensor, y_tensor_filepath)"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fJGqXolU3Ke",
        "outputId": "35721321-3e96-4fe5-999c-2d5e2426e056"
      },
      "source": [
        "# initialize inputs with respective data type\n",
        "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(x_tensor, y_tensor,\n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "BATCH_SIZE = 32\n",
        "tr_inputs = torch.tensor(tr_inputs,dtype=torch.float32)\n",
        "val_inputs = torch.tensor(val_inputs,dtype=torch.float32)\n",
        "tr_tags = torch.tensor(tr_tags,dtype=torch.float32)\n",
        "val_tags = torch.tensor(val_tags,dtype=torch.float32)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1VZ0WlGU3Kh"
      },
      "source": [
        "# setup tensor data set \n",
        "train_data = TensorDataset(tr_inputs, tr_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data,sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "valid_data = TensorDataset(val_inputs,val_tags)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data,sampler=valid_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK8BrPamU3Ki"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(2217, 300)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dout = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(300, 100)\n",
        "        self.prelu = nn.PReLU(1)\n",
        "        self.out = nn.Linear(100, 1)\n",
        "        self.out_act = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, input_):\n",
        "        a1 = self.fc1(input_)\n",
        "        h1 = self.relu1(a1)\n",
        "        dout = self.dout(h1)\n",
        "        a2 = self.fc2(dout)\n",
        "        h2 = self.prelu(a2)\n",
        "        a3 = self.out(h2)\n",
        "        y = self.out_act(a3)\n",
        "        return y\n",
        "    \n",
        "net = Net()\n",
        "opt = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
        "criterion = nn.BCELoss()"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-xBN3KPU3Kk"
      },
      "source": [
        "net.cuda();\n",
        "device = \"cuda\"\n",
        "EPOCHS = 7"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyLC2_lvU3Kl",
        "outputId": "15b04244-f789-4c4f-d2a5-c7d03c5503fb"
      },
      "source": [
        "net.train()\n",
        "for _ in range(EPOCHS):\n",
        "\n",
        "\n",
        "  total_loss = 0\n",
        "\n",
        "  for batch in train_dataloader:\n",
        "\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_x , b_labels = batch\n",
        "    opt.zero_grad()\n",
        "    y_hat = net(b_input_x)\n",
        "    loss = criterion(y_hat, b_labels.unsqueeze(1))\n",
        "    loss.backward()\n",
        "    opt.step()        \n",
        "    total_loss += loss.item()\n",
        "\n",
        "  avg_train_loss = total_loss / len(train_dataloader)\n",
        "  print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "  net.eval()\n",
        "  predictions,realtags = [],[]\n",
        "  for batch in valid_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_x , b_labels = batch\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions.\n",
        "        # This will return the logits rather than the loss because we have not provided labels.\n",
        "        outputs = net(b_input_x)\n",
        "\n",
        "    pred_labels = outputs.squeeze()>=0.5\n",
        "    pred_labels = pred_labels.detach().cpu().numpy()\n",
        "    real_labels = b_labels==1\n",
        "    real_labels = real_labels.detach().cpu().numpy()\n",
        "    predictions.extend(pred_labels)\n",
        "    realtags.extend(real_labels)\n",
        "  print(\"Validation Accuracy: {}\".format(accuracy_score(predictions, realtags)))"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 0.1425211300815047\n",
            "Validation Accuracy: 0.9759656652360515\n",
            "Average train loss: 0.07769159431394122\n",
            "Validation Accuracy: 0.8686695278969957\n",
            "Average train loss: 0.07163125208091692\n",
            "Validation Accuracy: 0.9789699570815451\n",
            "Average train loss: 0.0663990876982619\n",
            "Validation Accuracy: 0.9781115879828326\n",
            "Average train loss: 0.060919438164962515\n",
            "Validation Accuracy: 0.9802575107296138\n",
            "Average train loss: 0.05531164405129669\n",
            "Validation Accuracy: 0.9815450643776824\n",
            "Average train loss: 0.0559512019150777\n",
            "Validation Accuracy: 0.9815450643776824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "kgb9I03mU3Kn",
        "outputId": "6eef032e-029b-4365-8868-ac7b80b5e3fe"
      },
      "source": [
        "confusion_matrix = pd.DataFrame(confusion_matrix(realtags, predictions))\n",
        "confusion_matrix"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1916</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>14</td>\n",
              "      <td>371</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0    1\n",
              "0  1916   29\n",
              "1    14  371"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    }
  ]
}