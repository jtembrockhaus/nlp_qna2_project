{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "srl_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kzCROMSh3Ciy"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "88dc6c3f47674e38941cd9ece045d821": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ac17b2aee4a747e49f1745249c8f5182",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_78986ee903f840c89bc27d8196508185",
              "IPY_MODEL_3493aa4484f649a1aea8235dfd25e156"
            ]
          }
        },
        "ac17b2aee4a747e49f1745249c8f5182": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "78986ee903f840c89bc27d8196508185": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e9ddc2192f78433198a3f778ecb6c655",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1eab06d1c24a424cbb030b25a4f8aee6"
          }
        },
        "3493aa4484f649a1aea8235dfd25e156": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a5dcb56bebe1488381857b488a6d9c65",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 213k/213k [00:00&lt;00:00, 1.75MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c68af014b76a46ef86a0bcbaafb6aab2"
          }
        },
        "e9ddc2192f78433198a3f778ecb6c655": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1eab06d1c24a424cbb030b25a4f8aee6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a5dcb56bebe1488381857b488a6d9c65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c68af014b76a46ef86a0bcbaafb6aab2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ed7aa7926cf74c6ea2910abc40286c06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9ca29c9c639d49a292a495eb766590cb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1df5dc5c228442e6a7f8f2b001fdb8a5",
              "IPY_MODEL_67f1043fdd764c978165eb820be6c3f2"
            ]
          }
        },
        "9ca29c9c639d49a292a495eb766590cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1df5dc5c228442e6a7f8f2b001fdb8a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_af2948d5da9f497ebe2e09633c9e228c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 29,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 29,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7cc78f672c264930899c6ad9d25ec38a"
          }
        },
        "67f1043fdd764c978165eb820be6c3f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7808c5c1d2b6456faf53bc08e672bcda",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 29.0/29.0 [00:00&lt;00:00, 145B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_696af84d21ea43deaf9f0e934b81ba17"
          }
        },
        "af2948d5da9f497ebe2e09633c9e228c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7cc78f672c264930899c6ad9d25ec38a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7808c5c1d2b6456faf53bc08e672bcda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "696af84d21ea43deaf9f0e934b81ba17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9213949045ee452e82c1cbc6b3371366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4f6fac347abd496684e2ea708a64a719",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2213335f72d3407a92e0813f9e959f0d",
              "IPY_MODEL_af25ee675a8840f5b82273c14ffb2e29"
            ]
          }
        },
        "4f6fac347abd496684e2ea708a64a719": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2213335f72d3407a92e0813f9e959f0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_db71b4bf9f2a4a8989cce3c20a8dee4f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 435797,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435797,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6c8c4b209e1547c58b78e38c7f5c0e03"
          }
        },
        "af25ee675a8840f5b82273c14ffb2e29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_618d8409962542418d8ea1cab7d11279",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 436k/436k [00:00&lt;00:00, 5.23MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_833d162534414a0e887429303cba4219"
          }
        },
        "db71b4bf9f2a4a8989cce3c20a8dee4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6c8c4b209e1547c58b78e38c7f5c0e03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "618d8409962542418d8ea1cab7d11279": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "833d162534414a0e887429303cba4219": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "da58838027fe4960ad800b2d70b67d39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_249c167728fa46f1bc64a3634d0cfbbd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a15c9e7c18ca4c59b364e8f542486b2a",
              "IPY_MODEL_f061fd361cee4e50b3b6a12ecce4584e"
            ]
          }
        },
        "249c167728fa46f1bc64a3634d0cfbbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a15c9e7c18ca4c59b364e8f542486b2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e781db8e058a4d13a9511203e325d78b",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_91e75ec139da4022b69b719f38d77fd0"
          }
        },
        "f061fd361cee4e50b3b6a12ecce4584e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a3bfb2caa164463b821b2b8275bef16b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 6.41kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0de67ceaa0994c5aa4a8e21d471f6b24"
          }
        },
        "e781db8e058a4d13a9511203e325d78b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "91e75ec139da4022b69b719f38d77fd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a3bfb2caa164463b821b2b8275bef16b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0de67ceaa0994c5aa4a8e21d471f6b24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bbf804bc0f8b45459f8c1d21c42577e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b551d70eba2e4b449b362247f82d2b5d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_714c68495aae49bbb4f4cd9de9f514b8",
              "IPY_MODEL_282b215ba21844dd89843e592f994cfc"
            ]
          }
        },
        "b551d70eba2e4b449b362247f82d2b5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "714c68495aae49bbb4f4cd9de9f514b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_07a3a1fc407a491daa1b1e6e647ee18d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 435779157,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435779157,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0fa8c05b47d44db587bc13907a029cf2"
          }
        },
        "282b215ba21844dd89843e592f994cfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_556b772ed39048548c84ecc60bef2c6d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 436M/436M [00:08&lt;00:00, 51.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f012203da31b4fdeb5b268412ecddf0e"
          }
        },
        "07a3a1fc407a491daa1b1e6e647ee18d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0fa8c05b47d44db587bc13907a029cf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "556b772ed39048548c84ecc60bef2c6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f012203da31b4fdeb5b268412ecddf0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzCROMSh3Ciy"
      },
      "source": [
        "# Initialization "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z0Uz-7Y3HaT"
      },
      "source": [
        "## Install Packages\n",
        "At the beginning, packages neeed to be installed to execute the pipeline. The parameter `install_packages` can be set to True or False to indicate whether the required packages are already installed or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "CPxfApL_9u-V"
      },
      "source": [
        "install_packages = True #@param [\"True\", \"False\"] {type:\"raw\"}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7eufZDP3Gon",
        "outputId": "f5a83e8e-6034-4a69-ae08-dd156d29babc"
      },
      "source": [
        "if install_packages:\n",
        "  !pip install seqeval\n",
        "  !pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\r\u001b[K     |███████▌                        | 10kB 20.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 26.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30kB 31.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40kB 25.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16172 sha256=5eb55fc9367ce2d983783b279d42eb1cc99e4297801ed4f785e2236329bf95b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 18.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 42.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 40.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=f6c9d36d3b5519a359d7ed4c9bbb5c0d17d0fb71efc7dd453036a55d0852e41e\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzPgPpc5nJtq"
      },
      "source": [
        "## Import required Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfVYB9R4toT8"
      },
      "source": [
        "#GENERAL UTILITIES\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from progressbar import ProgressBar\n",
        "\n",
        "\n",
        "#IMPORTS FOR NEURAL NETWORK APPLICATION \n",
        "import transformers\n",
        "import transformers as ppb\n",
        "from transformers import BertModel, BertConfig, BertPreTrainedModel, AdamW, BertForTokenClassification\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "#IMPORTS FOR CREATING BROWN DATASET\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "#IMPORT FOR CREATING RUSSIAN FAIRYTALES DATASET\n",
        "import xml.dom.minidom\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "#IMPORTS FOR PREPROCESSING THE DATA\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#IMPORTS FOR EVALUATION OF PREDICTIONS\n",
        "from seqeval.metrics import f1_score, accuracy_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghc90HEZ3W79"
      },
      "source": [
        "## Mount Google Drive\n",
        "The user needs to give the notebook permission to access the google drive. The user needs to follow the link and copy paste the link into the field."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Shnt-31ytq_i",
        "outputId": "9465d54f-6649-4a68-d36b-a8459f3b7658"
      },
      "source": [
        "# mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inbmKBGM3xwQ"
      },
      "source": [
        "## Set the Working Directory\n",
        "This sets the parameter `working_dir`. This is the general working directory, the root directory for the notebook. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ljxgECDM3zCK"
      },
      "source": [
        "working_dir = \"/content/drive/path_to_unzipped_repository\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rEf-5DO4ZiR"
      },
      "source": [
        "# Semantic Role Labeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKHihXXreDOV"
      },
      "source": [
        "##  Setting Global Variables\n",
        "To ease the study of each used model, we set a few global parameters in the beginning of the notebook. \n",
        "The first parameter is `model_type`. It allows the user to set one of the five different neural network configurations:\n",
        "  1. `both_model_simple` a simple neural network linear layer ontop of bert contextualized embeddings trained on  both the fairy tale and brown dataset\n",
        "  2.`fairy_model_simple` a simple neural network linear layer ontop of bert contextualized embeddings trained on the fairy tale dataset\n",
        "  3.`fairy_model_complex` a complex neural network linear layer ontop of bert contextualized embeddings trained on the fairy tale dataset\n",
        "  4.`fairy_model_withAnim` a neural network linear layer ontop of bert contextualized embeddings in combination with animacy trained on the fairy tale dataset\n",
        "  5.`fairy_model_withPred` a neural network linear layer ontop of bert contextualized embeddings in combination with predicate indication trained on the fairy tale dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T99wbsu0dCLU",
        "cellView": "form"
      },
      "source": [
        "model_type = \"fairy_model_withAnim\" #@param [\"both_model_simple\", \"fairy_model_complex\", \"fairy_model_simple\", \"fairy_model_withAnim\", \"fairy_model_withPred\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjfKmMYWwhFM"
      },
      "source": [
        "Next is the parameter `fast_loading`. This parameter is defaulted to true and enables the loading of preprocessed dictionary files of the fairytale and brown datasets. These can be found on the github page under /input/data_dicts/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "2ygthR1Fgqr-"
      },
      "source": [
        "fast_loading = True #@param [\"True\", \"False\"] {type:\"raw\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPeY-5I8rTfc"
      },
      "source": [
        "## Loading Datasets - Fast Loading\n",
        "This can be done, if the preprocessed files are already\n",
        "available (default). The files are saved dictionaries in pickle format. We specify two input paths for the brown and fairytale dataset: `input_dir_brown` and `input_dir_fairytale` (if defaulted the user does not need to set anything). Both datasets are loaded into the environment. Depending on the model one into one dictionary called: `data_dict`. Each entry is one sentence with all the given annotations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2evhmkgDrCg5"
      },
      "source": [
        "input_dir_brown     = working_dir + 'data/srl_detection/input/data_dict_brown.pickle' \n",
        "input_dir_fairytale = working_dir + 'data/srl_detection/input/data_dict_fairytaile.pickle' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-zQHpX0qYw4"
      },
      "source": [
        "# Opens the dictionary files and saves them into data_dict\n",
        "\n",
        "if fast_loading:\n",
        "\n",
        "  with open(input_dir_brown, 'rb') as handle:\n",
        "      data_dict_brown = pickle.load(handle)\n",
        "  with open(input_dir_fairytale, 'rb') as handle:\n",
        "      data_dict_fairytale = pickle.load(handle)\n",
        "\n",
        "  data_dict = []\n",
        "\n",
        "  # Check for global variable and set data to specific model_type\n",
        "  if model_type == \"both_model_simple\":\n",
        "    data_dict.extend(data_dict_brown)\n",
        "    data_dict.extend(data_dict_fairytale)\n",
        "  else:\n",
        "    data_dict.extend(data_dict_fairytale)\n",
        "\n",
        "\n",
        "  compute_animacy = False\n",
        "  if model_type == \"fairy_model_withAnim\":\n",
        "    compute_animacy = True\n",
        "\n",
        "else:\n",
        "  print(\"Fast loading set to false, please provide the paths to the processed files otherwise default is used\")\n",
        "  try:\n",
        "    with open(input_dir_brown, 'rb') as handle:\n",
        "        data_dict_brown = pickle.load(handle)\n",
        "    with open(input_dir_fairytale, 'rb') as handle:\n",
        "        data_dict_fairytale = pickle.load(handle)\n",
        "\n",
        "    data_dict = []\n",
        "\n",
        "    # Check for global variable and set data to specific model_type\n",
        "    if model_type == \"both_model_simple\":\n",
        "      data_dict.extend(data_dict_brown)\n",
        "      data_dict.extend(data_dict_fairytale)\n",
        "    else:\n",
        "      data_dict.extend(data_dict_fairytale)\n",
        "\n",
        "\n",
        "    compute_animacy = False\n",
        "    if model_type == \"fairy_model_withAnim\":\n",
        "      compute_animacy = True\n",
        "  except IOError:\n",
        "    print(\"File not accessible\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iarM0jI9hyNF"
      },
      "source": [
        "Number of sentenences within the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s87Z_HjxqERP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3920aa26-4a38-4e7a-a5a1-4bfcbd2b765e"
      },
      "source": [
        "len(data_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3812"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSHFV29C1H9R"
      },
      "source": [
        "##  Processing - Create Tensors and Lableset\n",
        "This is the final Processing step to convert the individual sentences into a tensor data structure. \n",
        "\n",
        "The following steps are used:\n",
        "\n",
        "1. Creation of target class label dictionaries for predicate sense disambiguation and semantic arguments.\n",
        "2. Tokenization of target sentences by BERT from the `transformers` package.\n",
        "3. Padding of target tensors, creation of predicate indicators, animacy indicators and verb position indicators.\n",
        "4. Combining all individual tensor into one training dataset, where a train/test split is peformed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyNfINoNN9Li"
      },
      "source": [
        "### Processing - Step 1\n",
        "In this step we sweep over the given `data_dict`. For each sentence, three additional annotations are introduced:\n",
        "* `\"O\"` signifying words with missing annotations \n",
        "* `\"X\"` indicator for subtokens of words produced by BERT\n",
        "* `\"PAD\"` indicator for padded words of a sentence\n",
        "\n",
        "Furthermore each annotated sentence for semantic roles is converted according to the BIO-tagging scheme criteria. All target class labels are saved in two dictionaries, giving for each label the appropiate index and vice versa:\n",
        "\n",
        "* `tag2idx` , `idx2tag` are the predicate annotations\n",
        "* `bio2idx` , `idx2bio` are the semantic class labels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MqNdEDI1MUc"
      },
      "source": [
        "#Helper functions to peform step 1. Annotations of \"O\",\"X\" and \"PAD\".\n",
        "\n",
        "def create_set_list(data_dict,list_identifier):\n",
        "  only_preds = []\n",
        "  for index,value in enumerate(data_dict):\n",
        "    pred_list1 = value[list_identifier]\n",
        "    only_pred = list(filter(lambda a: a != \"O\", pred_list1))\n",
        "    only_preds.append(only_pred)\n",
        "  only_preds = [item for sublist in only_preds for item in sublist]\n",
        "  frame_labels = list(set(only_preds))\n",
        "  return frame_labels\n",
        "\n",
        "def create_dicts(label_list):\n",
        "  tag_values = [\"O\"]\n",
        "  tag_values.extend(list(sorted(label_list)))\n",
        "  tag_values.append(\"X\")\n",
        "  tag_values.append(\"PAD\")\n",
        "  tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
        "  candidate_labels_ids = tag2idx\n",
        "  idx2tag =  dict((v,k) for k,v in candidate_labels_ids.items())\n",
        "  return tag2idx,idx2tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2G6rMUm1MUd"
      },
      "source": [
        "#Sweeping over each sentence to convert semantic annotations according to the bio\n",
        "#tagging scheme criteria.\n",
        "for i in range(0,len(data_dict)):\n",
        "  first_token = data_dict[i][\"apred1\"][0]\n",
        "  if first_token == \"O\":\n",
        "    pass\n",
        "  else:\n",
        "    first_token = \"B-\" + first_token\n",
        "\n",
        "  bio_tagged_scheme = [first_token]\n",
        "  for index,current_val in enumerate(data_dict[i][\"apred1\"][1:]):\n",
        "    index    = index + 1\n",
        "    prev_val =  data_dict[i][\"apred1\"][(index-1)]\n",
        "    if current_val == \"O\" or current_val == \"[CLS]\" or current_val == \"[SEP]\":\n",
        "      pass\n",
        "    elif prev_val == current_val:\n",
        "      current_val = \"I-\" + current_val\n",
        "    else:\n",
        "      current_val = \"B-\" + current_val\n",
        "    bio_tagged_scheme.append(current_val)\n",
        "  data_dict[i][\"bio_tagged_list\"] = bio_tagged_scheme"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPg_JWjK1MUd"
      },
      "source": [
        "#Creating the class label sets for predicates and semantic roles\n",
        "bio2idx,idx2bio = create_dicts(create_set_list(data_dict,\"bio_tagged_list\"))\n",
        "tag_values = list(bio2idx.keys())\n",
        "tag2idx,idx2tag = create_dicts(create_set_list(data_dict,\"sense_list\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TKB3-qEPybx"
      },
      "source": [
        "### Processing - Step 2\n",
        "In this step we load the `tokenizer` function from the `BertModel` of the `tranformer` package. We use `\"bert-base-case\"` because it got the highest accuracy scores in our preliminary analysis. The `tokenizer` function splits individual words in to the word root and subtokens, called lemmatization. We tag each word root as its labeltag and each subtoken as `'X'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adgZAfYA1MUi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169,
          "referenced_widgets": [
            "88dc6c3f47674e38941cd9ece045d821",
            "ac17b2aee4a747e49f1745249c8f5182",
            "78986ee903f840c89bc27d8196508185",
            "3493aa4484f649a1aea8235dfd25e156",
            "e9ddc2192f78433198a3f778ecb6c655",
            "1eab06d1c24a424cbb030b25a4f8aee6",
            "a5dcb56bebe1488381857b488a6d9c65",
            "c68af014b76a46ef86a0bcbaafb6aab2",
            "ed7aa7926cf74c6ea2910abc40286c06",
            "9ca29c9c639d49a292a495eb766590cb",
            "1df5dc5c228442e6a7f8f2b001fdb8a5",
            "67f1043fdd764c978165eb820be6c3f2",
            "af2948d5da9f497ebe2e09633c9e228c",
            "7cc78f672c264930899c6ad9d25ec38a",
            "7808c5c1d2b6456faf53bc08e672bcda",
            "696af84d21ea43deaf9f0e934b81ba17",
            "9213949045ee452e82c1cbc6b3371366",
            "4f6fac347abd496684e2ea708a64a719",
            "2213335f72d3407a92e0813f9e959f0d",
            "af25ee675a8840f5b82273c14ffb2e29",
            "db71b4bf9f2a4a8989cce3c20a8dee4f",
            "6c8c4b209e1547c58b78e38c7f5c0e03",
            "618d8409962542418d8ea1cab7d11279",
            "833d162534414a0e887429303cba4219"
          ]
        },
        "outputId": "7a9d791a-74f5-4495-adb7-765e7c9e9710"
      },
      "source": [
        "#Initilize Bert Tokenizer\n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-cased')\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88dc6c3f47674e38941cd9ece045d821",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed7aa7926cf74c6ea2910abc40286c06",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9213949045ee452e82c1cbc6b3371366",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435797.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30l0lZqF1MUi"
      },
      "source": [
        "def tokenize_and_preserve_labels(sentence, text_labels, sense):\n",
        "    tokenized_sentence = []\n",
        "    labels = []\n",
        "\n",
        "    for word, label in zip(sentence, text_labels):\n",
        "\n",
        "        # Tokenize the word and count # of subwords the word is broken into\n",
        "        tokenized_word = tokenizer.tokenize(word)\n",
        "        n_subwords = len(tokenized_word)\n",
        "\n",
        "        # Add the tokenized word to the final tokenized word list\n",
        "        tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "        # Add the same label to the new list of labels `n_subwords` times\n",
        "\n",
        "        # Check for predicate or semantic role tokenization\n",
        "        if sense:\n",
        "          labels.extend([label])\n",
        "          if n_subwords > 1:\n",
        "            labels.extend([\"O\"] * (n_subwords-1))\n",
        "        else:\n",
        "          labels.extend([label])\n",
        "          if n_subwords > 1:\n",
        "            labels.extend([\"X\"] * (n_subwords-1))\n",
        "\n",
        "    return tokenized_sentence, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkiMHRTY1MUi"
      },
      "source": [
        "# Tokenize for predicate labels\n",
        "tokenized_texts_and_labels = [\n",
        "    tokenize_and_preserve_labels(curr_dict[\"tokens\"], curr_dict[\"sense_list\"],True)\n",
        "    for curr_dict in data_dict\n",
        "]\n",
        "\n",
        "# Tokenize for bio labels\n",
        "tokenized_texts_and_labels_bio = [\n",
        "    tokenize_and_preserve_labels(curr_dict[\"tokens\"], curr_dict[\"bio_tagged_list\"],False)\n",
        "    for curr_dict in data_dict\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qg_CbI58TqbT"
      },
      "source": [
        "# Sentence\n",
        "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
        "\n",
        "# Predicates\n",
        "labels          = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]\n",
        "\n",
        "# Semantic Roles\n",
        "bio_labels      = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels_bio]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkEqBQwuSaAr"
      },
      "source": [
        "### Processing - Step 3\n",
        "Next the longest sentence within the data set is identified. Saved in the variable `MAX_LENGTH`. Each sentence is converted to index integers and padded with the previously introduced `\"PAD\"` token. Furthermore we introduce the predicate embedding indicator (`pred_indicator`), animate embedding indicator (`anim_indicator`) and verb position indicator (`verb_indicator`). Both embedding indicators are simple random float vectors of length 10. Two seperate vectors indicating if the word is non animate/predicate or is animate/predicate.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4i73_dm1MUj"
      },
      "source": [
        "# Find the longest sentence within the data set and set to MAX_LENGTH\n",
        "MAX_LENGTH = 0\n",
        "for tupel in tokenized_texts_and_labels:\n",
        "  leng = int(len(tupel[0]))\n",
        "  if leng >= MAX_LENGTH:\n",
        "    MAX_LENGTH = leng"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q947JlUQ1MUj",
        "outputId": "761cf71d-58a2-4b8d-f044-8bfc8c288af0"
      },
      "source": [
        "print( \"The longest sentence is of length: \" +str(MAX_LENGTH))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The longest sentence is of length: 74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zrhsNWJ1MUk"
      },
      "source": [
        "# Convert word tokens to integers and pad to MAX_LENGTH\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAX_LENGTH, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9eyOXyc1MUk"
      },
      "source": [
        "#Convert word labels to integers and pad to MAX_LENGTH\n",
        "tt = [[tag2idx.get(l) if None != tag2idx.get(l) else (len(tag2idx)+1)  for l in lab] for lab in labels]\n",
        "tags = pad_sequences(tt,\n",
        "                     maxlen=MAX_LENGTH, value=tag2idx[\"PAD\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTu7JXDk1MUk"
      },
      "source": [
        "# Convert word semantic roles to integers and pad to MAX_LENGTH\n",
        "tt_bio = [[bio2idx.get(l) for l in lab] for lab in bio_labels]\n",
        "tags_bio = pad_sequences(tt_bio,\n",
        "                     maxlen=MAX_LENGTH, value=bio2idx[\"PAD\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uah4YCTx1MUm"
      },
      "source": [
        "# Find the word index for the predicates of the sentence\n",
        "verb_indicator = []\n",
        "\n",
        "# Iterate through each sentence\n",
        "for curr_tag in tags:\n",
        "  curr_indicator = np.zeros(MAX_LENGTH)\n",
        "\n",
        "  # Iterate through single sentence\n",
        "  for i,single_tags in enumerate(curr_tag):\n",
        "    # Check for predicate\n",
        "    if single_tags != tag2idx[\"PAD\"] and single_tags != tag2idx[\"O\"]:\n",
        "      curr_indicator[i] = 1\n",
        "      verb_indicator.append(curr_indicator)\n",
        "verb_indicator = np.array(verb_indicator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miuZbjypaWTu"
      },
      "source": [
        "# Find the word index for the predicates of the sentence and generate\n",
        "# a random vector of length 10. Pred_is indicates a predicate and pred_non\n",
        "# indicates a non predicate word.\n",
        "\n",
        "pred_is  = np.random.rand(10)\n",
        "pred_non = np.random.rand(10)\n",
        "pred_indicator = []\n",
        "\n",
        "# Iterate through each sentence\n",
        "for i,curr_tag in enumerate(tags):\n",
        "  one_tag = []\n",
        "\n",
        "\n",
        "  # Iterate through single sentence\n",
        "  for lables_int in curr_tag:\n",
        "\n",
        "    # Check for predicate\n",
        "    if lables_int != tag2idx[\"O\"] and lables_int != tag2idx[\"PAD\"]:\n",
        "      pred_ind = pred_is\n",
        "    else:\n",
        "      pred_ind = pred_non\n",
        "    one_tag.append(pred_ind)\n",
        "  pred_indicator.append(one_tag)\n",
        "pred_indicator      = np.array(pred_indicator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvQaBi4rxtxV"
      },
      "source": [
        "# Find the word index for the animate words of the sentence and generate\n",
        "# a random vector of length 10. anim_is indicates a animate word and anim_non\n",
        "# indicates a non animate word.\n",
        "\n",
        "\n",
        "if compute_animacy:\n",
        "\n",
        "  tokenized_texts_and_labels_anim = [\n",
        "      tokenize_and_preserve_labels(curr_dict[\"tokens\"], curr_dict[\"animacy\"],True)\n",
        "      for curr_dict in data_dict\n",
        "  ]\n",
        "\n",
        "\n",
        "  anim_labels     = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels_anim]\n",
        "\n",
        "  #Need to define a sublabeling set just for animacy\n",
        "  anim2idx = {\"O\":0,\"A\":1,\"PAD\":2}\n",
        "  candidate_labels_ids = anim2idx\n",
        "  idx2anim =  dict((v,k) for k,v in candidate_labels_ids.items())\n",
        "\n",
        "  tt_anim = [[anim2idx.get(l) for l in lab] for lab in anim_labels]\n",
        "\n",
        "  tags_anim = pad_sequences(tt_anim,\n",
        "                      maxlen=MAX_LENGTH, value=anim2idx[\"PAD\"], padding=\"post\",\n",
        "                      dtype=\"long\", truncating=\"post\")\n",
        "\n",
        "  #Generate the final anim_indicator embeddings\n",
        "  anim_is = np.random.rand(10)\n",
        "  anim_non = np.random.rand(10)\n",
        "  anim_indicator = []\n",
        "\n",
        "  #Iterate through each sentence \n",
        "  for i,curr_tag in enumerate(tags_anim):\n",
        "    one_tag = []\n",
        "\n",
        "    #Iterate through single sentence\n",
        "    for lables_int in curr_tag:\n",
        "\n",
        "      #Check for animacy\n",
        "      if lables_int == anim2idx[\"A\"]:\n",
        "        anim_ind = anim_is\n",
        "      else:\n",
        "        anim_ind = anim_non\n",
        "      one_tag.append(anim_ind)\n",
        "    anim_indicator.append(one_tag)\n",
        "  anim_indicator      = np.array(anim_indicator)\n",
        "else:\n",
        "  anim_indicator = pred_indicator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCH5hxxPYeXO"
      },
      "source": [
        "### Processing - Step 4\n",
        "Finally all datasets are converted to tensors and combined into one final training dataset. A $90\\%$ to $10\\%$ split into training and test is peformed. Additionally an attentionmask is introduced, which is a boolean vector with true for given words in a sentence and 0 for pad tokens. The final datasets variables for the neural network are `train_dataloader` and `valid_dataloader`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2No2UbU1MUm"
      },
      "source": [
        "# Create attention mask vector\n",
        "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCbktS871MUm"
      },
      "source": [
        "\n",
        "# Peform the same train test split for each individual data class\n",
        "\n",
        "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags,\n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)\n",
        "tr_tags_bio, val_tags_bio, _, _ = train_test_split(tags_bio, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)\n",
        "tr_verb_indicator, val_verb_indicator, _, _ = train_test_split(verb_indicator, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)\n",
        "tr_anim_indicator, val_anim_indicator, _, _ = train_test_split(anim_indicator, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)\n",
        "tr_pred_indicator, val_pred_indicator, _, _ = train_test_split(pred_indicator, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2Rm3aF41MUm"
      },
      "source": [
        "# Create the tensor vectors\n",
        "\n",
        "tr_inputs = torch.tensor(tr_inputs,dtype=torch.long)\n",
        "val_inputs = torch.tensor(val_inputs,dtype=torch.long)\n",
        "tr_tags = torch.tensor(tr_tags,dtype=torch.long)\n",
        "val_tags = torch.tensor(val_tags,dtype=torch.long)\n",
        "tr_tags_bio = torch.tensor(tr_tags_bio,dtype=torch.long)\n",
        "val_tags_bio = torch.tensor(val_tags_bio,dtype=torch.long)\n",
        "tr_masks = torch.tensor(tr_masks,dtype=torch.long)\n",
        "val_masks = torch.tensor(val_masks,dtype=torch.long)\n",
        "tr_verb_indicator = torch.tensor(tr_verb_indicator,dtype=torch.long)\n",
        "val_verb_indicator = torch.tensor(val_verb_indicator,dtype=torch.long)\n",
        "tr_anim_indicator = torch.tensor(tr_anim_indicator,dtype=torch.long)\n",
        "val_anim_indicator = torch.tensor(val_anim_indicator,dtype=torch.long)\n",
        "tr_pred_indicator = torch.tensor(tr_pred_indicator,dtype=torch.long)\n",
        "val_pred_indicator = torch.tensor(val_pred_indicator,dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB1CKcp-1MUm"
      },
      "source": [
        "# Loading tensors into one datasets and initilize the random sampling\n",
        "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags,tr_tags_bio,tr_verb_indicator,tr_anim_indicator,tr_pred_indicator)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "valid_data = TensorDataset(val_inputs, val_masks, val_tags,val_tags_bio,val_verb_indicator,val_anim_indicator,val_pred_indicator)\n",
        "valid_sampler = SequentialSampler(valid_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBm5PNMmWqp4"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "## Neural Network Application\n",
        "The general outline of the model follows the paper by Shi, Peng, and Jimmy Lin: *Simple bert models for relation extraction and semantic role labeling.*\n",
        "\n",
        "The neural network consists of a BERT contextual embedder in combination with a linear layer for classification. \n",
        "BERT converts the target data into numerical contextual embeddings, where the semantic relations within the sentence are preserved. Each word is converted to a tensor of 768 numerical floats. Thus a target dataset converts to a tensor with dimensions $(sentence_{all},sentence_{length},768)$. If we use predicate or animate indicators the last dimensions increases by $10$ i.e. $768+10 = 778$. Our linear classification layers are of size $n = len(labelset)$.\n",
        "Three parameters we deemed most important can be set as global variables:\n",
        " 1. `BATCH_SIZE`: How many data points are shown to the neural network at once. \n",
        " 2. `EPOCH`: One whole pass over the dataset\n",
        " 3. `learning_rate`: Step size until convergence to a minimum\n",
        " \n",
        "Within the literature a `BATCH_SIZE` of $32$ was deemed optimal.\n",
        "In our analysis we used an `EPOCH` value of $15$. Smaller values are suitable too due to diminishing returns after $8$ epochs. The `learning_rate` is set to $0.00003$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "_qv7KD_9R_4C"
      },
      "source": [
        "BATCH_SIZE = 32 #@param {type:\"integer\"}\n",
        "EPOCHS     =   15#@param {type:\"integer\"}\n",
        "learning_rate = 3e-5   #@param {type:\"number\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9hSzAY-Yrzd"
      },
      "source": [
        "# Initializes the data loader at the specified batch size\n",
        "train_dataloader = DataLoader(train_data,sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "valid_dataloader = DataLoader(valid_data,sampler=valid_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbobEXtNYtBz"
      },
      "source": [
        "### Baseclass Overwrite \n",
        "We use the base class `BertForTokenClassification` from the `transformers` package. It includes the BERT layer and a simple linear layer stacked ontop of the embeddings. By loading the `BertForTokenClassification` into the environment we can make small modifications to the model to include our specific use case. Mainly we added more complexity i.e. linear layers within the `BertForTokenClassificationComplex` class and we added predicate/animate indication within the `BertForTokenClassificationInd` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TjczvFQkKC3"
      },
      "source": [
        "class BertForTokenClassificationInd(BertPreTrainedModel):\n",
        "\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        # Number of unique target labels\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        # Init bert and linear layers with dropout and hidden layer sizes\n",
        "        self.bert       = BertModel(config, add_pooling_layer=False)\n",
        "        self.dropout    = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size + 10, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "\n",
        "        #Input sentence\n",
        "        input_ids=None,\n",
        "\n",
        "        #Input attenmask\n",
        "        attention_mask=None,\n",
        "\n",
        "        #Whats the second sentence, in our case the predicate\n",
        "        token_type_ids=None,\n",
        "\n",
        "        #Not relevant\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "\n",
        "        #Target labels\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "\n",
        "        #Target indicators\n",
        "        embedder_indicator=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n",
        "            1]``.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "\n",
        "        # Bert outputs \n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        # Concatenate the indicators to the output\n",
        "        sequence_output = torch.cat((sequence_output,embedder_indicator),dim=2)\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "\n",
        "        # Forward pass through the network to compute logits\n",
        "        batch_size,sequence_length,embedding_dim = sequence_output.size()\n",
        "        logits                 = self.classifier(sequence_output)\n",
        "\n",
        "\n",
        "        # If labels available compute loss for gradient computation \n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            # Only keep active parts of the loss\n",
        "            if attention_mask is not None:\n",
        "                active_loss = attention_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, self.num_labels)\n",
        "                active_labels = torch.where(\n",
        "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "                )\n",
        "                loss = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return loss,logits\n",
        "\n",
        "\n",
        "class BertForTokenClassificationComplex(BertPreTrainedModel):\n",
        "\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "\n",
        "        # Init of all bert and multiple layers with hidden size 300\n",
        "        self.bert       = BertModel(config, add_pooling_layer=False)\n",
        "        self.dropout    = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.dropout_hidden   = nn.Dropout(0.2)\n",
        "        self.hidden_layer = nn.Linear(config.hidden_size, 300)\n",
        "        self.classifier   = nn.Linear(300 , config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        #Input sentence\n",
        "        input_ids=None,\n",
        "\n",
        "        #Input attenmask\n",
        "        attention_mask=None,\n",
        "\n",
        "        #Whats the second sentence, in our case the predicate\n",
        "        token_type_ids=None,\n",
        "\n",
        "        #Not relevant\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "\n",
        "        #Target labels\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "\n",
        "        #Target indicators\n",
        "        embedder_indicator=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n",
        "            1]``.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        # Forward pass through the linear layers, with added linear layers\n",
        "        sequence_output = outputs[0]\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        batch_size,sequence_length,embedding_dim = sequence_output.size()\n",
        "        sequence_output        = self.hidden_layer(sequence_output)\n",
        "        sequence_output        = F.relu(sequence_output)\n",
        "        sequence_output        = self.dropout_hidden(sequence_output)\n",
        "        logits                 = self.classifier(sequence_output)\n",
        "\n",
        "\n",
        "        # Compute loss if labels are available\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            # Only keep active parts of the loss\n",
        "            if attention_mask is not None:\n",
        "                active_loss = attention_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, self.num_labels)\n",
        "                active_labels = torch.where(\n",
        "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "                )\n",
        "                loss = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return loss,logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232,
          "referenced_widgets": [
            "da58838027fe4960ad800b2d70b67d39",
            "249c167728fa46f1bc64a3634d0cfbbd",
            "a15c9e7c18ca4c59b364e8f542486b2a",
            "f061fd361cee4e50b3b6a12ecce4584e",
            "e781db8e058a4d13a9511203e325d78b",
            "91e75ec139da4022b69b719f38d77fd0",
            "a3bfb2caa164463b821b2b8275bef16b",
            "0de67ceaa0994c5aa4a8e21d471f6b24",
            "bbf804bc0f8b45459f8c1d21c42577e8",
            "b551d70eba2e4b449b362247f82d2b5d",
            "714c68495aae49bbb4f4cd9de9f514b8",
            "282b215ba21844dd89843e592f994cfc",
            "07a3a1fc407a491daa1b1e6e647ee18d",
            "0fa8c05b47d44db587bc13907a029cf2",
            "556b772ed39048548c84ecc60bef2c6d",
            "f012203da31b4fdeb5b268412ecddf0e"
          ]
        },
        "id": "xZFvlT0mkVXT",
        "outputId": "0194b7af-2cf7-447c-f2e4-b2799b12949d"
      },
      "source": [
        "\n",
        "#Initilize the specific model based on the global variable model_type\n",
        "\n",
        "if model_type == \"both_model_simple\" or model_type == \"fairy_model_simple\":\n",
        "  model = BertForTokenClassification.from_pretrained(\n",
        "      \"bert-base-cased\",\n",
        "      num_labels=len(bio2idx),\n",
        "      output_attentions = False,\n",
        "      output_hidden_states = False\n",
        "  )\n",
        "\n",
        "elif model_type == \"fairy_model_withAnim\" or model_type == \"fairy_model_withPred\":\n",
        "  model = BertForTokenClassificationInd.from_pretrained(\n",
        "      \"bert-base-cased\",\n",
        "      num_labels=len(bio2idx),\n",
        "      output_attentions = False,\n",
        "      output_hidden_states = False\n",
        "  )\n",
        "\n",
        "elif model_type == \"fairy_model_complex\":\n",
        "  model = BertForTokenClassificationComplex.from_pretrained(\n",
        "      \"bert-base-cased\",\n",
        "      num_labels=len(bio2idx),\n",
        "      output_attentions = False,\n",
        "      output_hidden_states = False\n",
        "  )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da58838027fe4960ad800b2d70b67d39",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbf804bc0f8b45459f8c1d21c42577e8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassificationInd: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassificationInd from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassificationInd from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassificationInd were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8W_khxglaSq"
      },
      "source": [
        "\n",
        "#The following codesnippets are based on the given tutorial by:\n",
        "#https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/\n",
        "\n",
        "#Weight decay is a regulization method and a penilization method for complexity\n",
        "#of the model. Furthermore we use the AdamW optimizer for the best optimization.\n",
        "\n",
        "\n",
        "FULL_FINETUNING = True\n",
        "if FULL_FINETUNING:\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "else:\n",
        "    param_optimizer = list(model.classifier.named_parameters())\n",
        "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "\n",
        "optimizer = AdamW(\n",
        "    optimizer_grouped_parameters,\n",
        "    lr=learning_rate,\n",
        "    eps=1e-8\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bDZhG3alaSr"
      },
      "source": [
        "max_grad_norm = 1.0\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * EPOCHS\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "model.cuda();\n",
        "device = \"cuda\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB0dr72dOU8X"
      },
      "source": [
        "## Training\n",
        "This is the final training and validation step of the initialized neural networks. We iterate over the whole dataset `EPOCHS` times while presenting the model `BATCH_SIZE` sentences. Each individual step of the training is commented within the code cell, but follows this general outline:\n",
        "\n",
        " * Loop over `EPOCHS` and set model into training mode\n",
        "  * Loop over `BATCH_SIZE` of training data\n",
        "    *  Calculate forward pass through model\n",
        "    *  Calculate loss and peform backward pass to calculate gradients\n",
        "    *  Update paramters with optimizer and update learning rate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-IhPOC4laSs",
        "outputId": "4d1e9195-f6e0-4391-ca97-2aa3c746e20c"
      },
      "source": [
        "## Store the average loss after each epoch so we can plot them.\n",
        "loss_values, validation_loss_values, val_accuracies = [], [], []\n",
        "\n",
        "for _ in trange(EPOCHS, desc=\"Epoch\"):\n",
        "\n",
        "    # TRAINING\n",
        "\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    # Put the model into training mode.\n",
        "    model.train()\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Training loop\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Add batch to gpu\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, _,b_labels,b_frame_indicator,b_anim_indicator,b_pred_indicator  = batch\n",
        "\n",
        "\n",
        "        # Check for global variable to initilize the specfic model\n",
        "        if model_type == \"fairy_model_withAnim\": embedder_indicator = b_anim_indicator \n",
        "        if model_type == \"fairy_model_withPred\": embedder_indicator = b_pred_indicator \n",
        "\n",
        "        # Check for global variable to initilize the specfic model\n",
        "        if model_type == \"both_model_simple\" or model_type == \"fairy_model_simple\":\n",
        "          outputs = model(b_input_ids, token_type_ids=b_frame_indicator,attention_mask=b_input_mask, labels=b_labels)\n",
        "          loss = outputs[0]\n",
        "        else:\n",
        "          loss,logits = model(b_input_ids, token_type_ids=b_frame_indicator,attention_mask=b_input_mask, labels=b_labels,embedder_indicator=embedder_indicator)\n",
        "        \n",
        "        # Always clear any previously calculated gradients before performing a backward pass.\n",
        "        model.zero_grad()\n",
        "        \n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        # track train loss\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        # Clip the norm of the gradient\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "\n",
        "\n",
        "    # VALIDATION\n",
        "\n",
        "\n",
        "    # Put the model into evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Reset the validation loss for this epoch.\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    predictions , true_labels = [], []\n",
        "    for batch in valid_dataloader:\n",
        "\n",
        "        # Add batch to gpu\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, _,b_labels,b_frame_indicator,b_anim_indicator,b_pred_indicator  = batch\n",
        "\n",
        "        # Check for global variable to initilize the specfic model\n",
        "        if model_type == \"fairy_model_withAnim\": embedder_indicator = b_anim_indicator \n",
        "        if model_type == \"fairy_model_withPred\": embedder_indicator = b_pred_indicator \n",
        "\n",
        "\n",
        "        # Check for global variable to initilize the specfic model\n",
        "        with torch.no_grad():\n",
        "            if model_type == \"both_model_simple\" or model_type == \"fairy_model_simple\":\n",
        "              outputs = model(b_input_ids, token_type_ids=b_frame_indicator,attention_mask=b_input_mask, labels=b_labels)\n",
        "              loss    = outputs[0]\n",
        "              logits  = outputs[1]\n",
        "            else:\n",
        "              loss,logits = model(b_input_ids, token_type_ids=b_frame_indicator,attention_mask=b_input_mask, labels=b_labels,embedder_indicator=embedder_indicator)\n",
        "        \n",
        "        # Move logits and labels to CPU\n",
        "        logits    = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        eval_loss += loss.mean().item()\n",
        "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "        true_labels.extend(label_ids)\n",
        "\n",
        "\n",
        "    # Calculate the accuracy for the whole epoch. Save predictions in \n",
        "    # list for later access.\n",
        "    eval_loss = eval_loss / len(valid_dataloader)\n",
        "    validation_loss_values.append(eval_loss)\n",
        "    print(\"Validation loss: {}\".format(eval_loss))\n",
        "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
        "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
        "    valid_tags = [tag_values[l_i] for l in true_labels\n",
        "                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n",
        "    print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
        "    val_accuracies.append(accuracy_score(pred_tags, valid_tags))\n",
        "\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/15 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 1.3287757574408143\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   7%|▋         | 1/15 [00:46<10:56, 46.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.8648726840813955\n",
            "Validation Accuracy: 0.7841269841269841\n",
            "\n",
            "Average train loss: 0.7207662002355965\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  13%|█▎        | 2/15 [01:35<10:17, 47.49s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.5690808420379957\n",
            "Validation Accuracy: 0.84\n",
            "\n",
            "Average train loss: 0.4779277957148022\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  20%|██        | 3/15 [02:23<09:31, 47.65s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.43902693192164105\n",
            "Validation Accuracy: 0.8791534391534391\n",
            "\n",
            "Average train loss: 0.3418442108840854\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  27%|██▋       | 4/15 [03:12<08:46, 47.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.3676605820655823\n",
            "Validation Accuracy: 0.9013756613756614\n",
            "\n",
            "Average train loss: 0.26638245258342336\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  33%|███▎      | 5/15 [04:00<08:00, 48.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.3207383304834366\n",
            "Validation Accuracy: 0.9082539682539682\n",
            "\n",
            "Average train loss: 0.20940720752157546\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  40%|████      | 6/15 [04:49<07:13, 48.15s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.30398569876948994\n",
            "Validation Accuracy: 0.917989417989418\n",
            "\n",
            "Average train loss: 0.1729775776879655\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  47%|████▋     | 7/15 [05:37<06:25, 48.23s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.3011486480633418\n",
            "Validation Accuracy: 0.921058201058201\n",
            "\n",
            "Average train loss: 0.14789561437511886\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  53%|█████▎    | 8/15 [06:25<05:38, 48.30s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.2823372036218643\n",
            "Validation Accuracy: 0.9267724867724868\n",
            "\n",
            "Average train loss: 0.12679987207606988\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  60%|██████    | 9/15 [07:14<04:50, 48.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.2792387157678604\n",
            "Validation Accuracy: 0.9296296296296296\n",
            "\n",
            "Average train loss: 0.10900005367067125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  67%|██████▋   | 10/15 [08:02<04:01, 48.36s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.2814231564601262\n",
            "Validation Accuracy: 0.9307936507936508\n",
            "\n",
            "Average train loss: 0.09468761531429158\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  73%|███████▎  | 11/15 [08:51<03:13, 48.36s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.2947673213978608\n",
            "Validation Accuracy: 0.9306878306878307\n",
            "\n",
            "Average train loss: 0.08348076833687999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  80%|████████  | 12/15 [09:39<02:25, 48.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.30159852902094525\n",
            "Validation Accuracy: 0.9312169312169312\n",
            "\n",
            "Average train loss: 0.07656184059602243\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  87%|████████▋ | 13/15 [10:27<01:36, 48.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.2971922419965267\n",
            "Validation Accuracy: 0.9314285714285714\n",
            "\n",
            "Average train loss: 0.06974895236392815\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  93%|█████████▎| 14/15 [11:16<00:48, 48.33s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.29927175864577293\n",
            "Validation Accuracy: 0.9315343915343915\n",
            "\n",
            "Average train loss: 0.06747123348975072\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 15/15 [12:04<00:00, 48.29s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.2975124195218086\n",
            "Validation Accuracy: 0.9308994708994709\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHKtHydQ1bFT"
      },
      "source": [
        "## Save Model and Results\n",
        "In this step of the notebook all results are saved into files. The trained model is saved into the folder structure /models/. The validation file containing the accuray and loss scorings as well as a confusion matrix file are saved in . "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3zdBnzWnTHG"
      },
      "source": [
        "# Create results dataframe\n",
        "temp_dict = {\"validation_accuracies\":val_accuracies,\"loss_value\":loss_values,\"validation_loss_value\":validation_loss_values,\"model\":model_type}\n",
        "temp_data_df = pd.DataFrame(temp_dict)\n",
        "\n",
        "# Save validation data results\n",
        "output_validation_data = working_dir + \"data/srl_detection/output/\" + model_type + \"_validation_results.csv\"\n",
        "temp_data_df.to_csv(output_validation_data, index=False) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "_AM6xKt-Btrq",
        "outputId": "1a2179af-a1f4-45aa-9c53-c6642ab65974"
      },
      "source": [
        "temp_data_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>validation_accuracies</th>\n",
              "      <th>loss_value</th>\n",
              "      <th>validation_loss_value</th>\n",
              "      <th>model</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.784127</td>\n",
              "      <td>1.328776</td>\n",
              "      <td>0.864873</td>\n",
              "      <td>fairy_model_withAnim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.840000</td>\n",
              "      <td>0.720766</td>\n",
              "      <td>0.569081</td>\n",
              "      <td>fairy_model_withAnim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.879153</td>\n",
              "      <td>0.477928</td>\n",
              "      <td>0.439027</td>\n",
              "      <td>fairy_model_withAnim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.901376</td>\n",
              "      <td>0.341844</td>\n",
              "      <td>0.367661</td>\n",
              "      <td>fairy_model_withAnim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.908254</td>\n",
              "      <td>0.266382</td>\n",
              "      <td>0.320738</td>\n",
              "      <td>fairy_model_withAnim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.917989</td>\n",
              "      <td>0.209407</td>\n",
              "      <td>0.303986</td>\n",
              "      <td>fairy_model_withAnim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.921058</td>\n",
              "      <td>0.172978</td>\n",
              "      <td>0.301149</td>\n",
              "      <td>fairy_model_withAnim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.926772</td>\n",
              "      <td>0.147896</td>\n",
              "      <td>0.282337</td>\n",
              "      <td>fairy_model_withAnim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.929630</td>\n",
              "      <td>0.126800</td>\n",
              "      <td>0.279239</td>\n",
              "      <td>fairy_model_withAnim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.930794</td>\n",
              "      <td>0.109000</td>\n",
              "      <td>0.281423</td>\n",
              "      <td>fairy_model_withAnim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.930688</td>\n",
              "      <td>0.094688</td>\n",
              "      <td>0.294767</td>\n",
              "      <td>fairy_model_withAnim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.931217</td>\n",
              "      <td>0.083481</td>\n",
              "      <td>0.301599</td>\n",
              "      <td>fairy_model_withAnim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.931429</td>\n",
              "      <td>0.076562</td>\n",
              "      <td>0.297192</td>\n",
              "      <td>fairy_model_withAnim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.931534</td>\n",
              "      <td>0.069749</td>\n",
              "      <td>0.299272</td>\n",
              "      <td>fairy_model_withAnim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.930899</td>\n",
              "      <td>0.067471</td>\n",
              "      <td>0.297512</td>\n",
              "      <td>fairy_model_withAnim</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    validation_accuracies  ...                 model\n",
              "0                0.784127  ...  fairy_model_withAnim\n",
              "1                0.840000  ...  fairy_model_withAnim\n",
              "2                0.879153  ...  fairy_model_withAnim\n",
              "3                0.901376  ...  fairy_model_withAnim\n",
              "4                0.908254  ...  fairy_model_withAnim\n",
              "5                0.917989  ...  fairy_model_withAnim\n",
              "6                0.921058  ...  fairy_model_withAnim\n",
              "7                0.926772  ...  fairy_model_withAnim\n",
              "8                0.929630  ...  fairy_model_withAnim\n",
              "9                0.930794  ...  fairy_model_withAnim\n",
              "10               0.930688  ...  fairy_model_withAnim\n",
              "11               0.931217  ...  fairy_model_withAnim\n",
              "12               0.931429  ...  fairy_model_withAnim\n",
              "13               0.931534  ...  fairy_model_withAnim\n",
              "14               0.930899  ...  fairy_model_withAnim\n",
              "\n",
              "[15 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CRTPY1lnTHH"
      },
      "source": [
        "# Create and save confusion matrix file\n",
        "confusion_matrix_both = pd.DataFrame(confusion_matrix(valid_tags, pred_tags, labels=tag_values))\n",
        "output_confusion_matrix = working_dir + \"data/srl_detection/output/\" + model_type + \"_confusion_matrix.cvs\"\n",
        "confusion_matrix_both.to_csv(output_confusion_matrix,index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvWsDHvWlaSt"
      },
      "source": [
        "# Save the trained model into file\n",
        "output_trained_model = working_dir + \"models/\" + model_type + \"_trained\"\n",
        "torch.save(model.state_dict(),output_trained_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9vFBB9C56Ra"
      },
      "source": [
        "# Trained Model Application\n",
        "This is the final step of the notebook. Here we showcase our best peforming previously trained models. First we load both into the environment. Next we make a final prediction on target sentences from mainstream literature not yet presented to the models. \n",
        "Unfortunately, the entire preprocessed training model exceeds the maximum file size allowed by github. The required file can be manually downloaded by clicking on the following link: https://drive.google.com/u/0/uc?export=download&confirm=S9Jm&id=1-CpTgM7WfSPdpNnEFEignMgWheFwrbue\n",
        "\n",
        "Since Google implemented an extra information that large files can not be scanned for viruses, the download can not be automated and performed by wget. The file needs to be saved under `\"./data/srl_detection/input/\"`. After the download is completed, one can continue\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9HkUElllaSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b247494-8325-410d-dd41-99949caf8180"
      },
      "source": [
        "# Loading the pretained animacy neural network model\n",
        "# First the defined class needs to be loaded into environment\n",
        "class Net(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "      self.fc1 = nn.Linear(1582, 300)\n",
        "      self.relu1 = nn.ReLU()\n",
        "      self.dout = nn.Dropout(0.2)\n",
        "      self.fc2 = nn.Linear(300, 100)\n",
        "      self.prelu = nn.PReLU(1)\n",
        "      self.out = nn.Linear(100, 1)\n",
        "      self.out_act = nn.Sigmoid()\n",
        "      \n",
        "  def forward(self, input_):\n",
        "      a1 = self.fc1(input_)\n",
        "      h1 = self.relu1(a1)\n",
        "      dout = self.dout(h1)\n",
        "      a2 = self.fc2(dout)\n",
        "      h2 = self.prelu(a2)\n",
        "      a3 = self.out(h2)\n",
        "      y = self.out_act(a3)\n",
        "      return y\n",
        "  \n",
        "net = Net()\n",
        "input_animate   = working_dir + \"models/AnimacyDetection_MLP_model\"\n",
        "input_model_srl = working_dir + \"data/srl_detection/input/\"\n",
        "\n",
        "# Loading the model\n",
        "net.load_state_dict(torch.load(input_animate, map_location='cpu'))\n",
        "net.eval()\n",
        "\n",
        "# Loading the class\n",
        "model = BertForTokenClassification.from_pretrained(\n",
        "    \"bert-base-cased\",\n",
        "    num_labels=52,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False\n",
        ")\n",
        "\n",
        "# Loading the model\n",
        "\n",
        "input_model_srl = input_model_srl + \"both_model_simple\"\n",
        "model.load_state_dict(torch.load(input_model_srl, map_location='cpu'))\n",
        "model.eval()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=52, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CMsbcg0laSu"
      },
      "source": [
        "#Load tokenizer class\n",
        "model_class, tokenizer_class, pretrained_weights = (transformers.BertModel, transformers.BertTokenizer, 'bert-base-cased')\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT7nOdAflaSu"
      },
      "source": [
        "# Example maintstream sentences with predicate index vector\n",
        "one_liners = [\n",
        "(\"When you play the game of thrones, you win or you die.\",[0,0,1,0,0,0,0,0,0,0,0,0,0,0,0]),  #GOT ONE LINERS\n",
        "(\"A man with no motive is a man no one suspects.\",[0,0,0,0,0,0,0,0,0,0,1,0],[0,0,0,0,0,1,0,0,0,0,0,0]),#GOT ONE LINERS\n",
        "(\"Now he realised the cruelty of his gift.\",[0,0,1,0,0,0,0,0,0]),                            #GREEK MYTHS\n",
        "(\"Even the smallest person can change the course of the future.\",[0,0,0,0,0,1,0,0,0,0,0,0]) ,#LORD OF THE RINGS\n",
        "(\"No other country has counted so many deaths in the pandemic.\",[0,0,0,0,1,0,0,0,0,0,0,0,0,0]),\n",
        "(\"A talking tree sits on a bench and looks at a dog.\",[0,0,0,0,0,0,0,0,1,0,0,0,0])]#NEW YOKR TIMES ARTICLE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rJE56I_8pgD"
      },
      "source": [
        "# Helper function to predict a sentence using a pretained model\n",
        "def predicte_sentence(sentence,pred_ind):\n",
        "    tokenized_sentence = tokenizer.encode(sentence,add_special_tokens=False)\n",
        "    input_ids          = torch.tensor([tokenized_sentence]).cpu()\n",
        "    verb_indicator     = torch.tensor([pred_ind]).cpu()\n",
        "    outputs = model(input_ids,token_type_ids=verb_indicator)\n",
        "    logits = outputs[0].detach().cpu().numpy()\n",
        "    test = [list(p) for p in np.argmax(logits, axis=2)]\n",
        "    pred_tags = [tag_values[p_i] for p in test for p_i in p]\n",
        "    return (tokenizer.tokenize(sentence),pred_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow2oB2XFlaSy",
        "outputId": "4204f067-2920-430f-ed23-4b6f6849c364"
      },
      "source": [
        "\n",
        "# Due to the highly comples preprocessing of the Animacy steps we use \n",
        "# the preprocessed files of the given target sentences and load these \n",
        "# into the environment\n",
        "input_path = working_dir + \"data/srl_detection/input/\"\n",
        "all_data = []\n",
        "for i in range(6):\n",
        "    file_name = input_path + \"sentence\" + str(i) + \".pt\"\n",
        "    all_data.append(file_name)\n",
        "\n",
        "\n",
        "pre_one_liners = []\n",
        "\n",
        "for i in all_data:\n",
        "    test1          =  torch.load(i)\n",
        "    tr_inputs      = torch.tensor(test1,dtype=torch.float32)\n",
        "    pre_one_liners.append(tr_inputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DKZV8RGlaSy"
      },
      "source": [
        "def classifier_final(tr_inputs,tokens,predindicator):\n",
        "    tokens,preds = predicte_sentence(tokens,predindicator)\n",
        "    output       = net(tr_inputs)\n",
        "    pred_labels  = output.squeeze()>=0.5\n",
        "    pred_labels  = pred_labels.detach().cpu().numpy()\n",
        "    index = 0\n",
        "    for i,j in zip(tokens,preds):\n",
        "\n",
        "        if i.startswith(\"#\"):\n",
        "            anim = \"O\"\n",
        "        else:\n",
        "            if pred_labels[index] == True:\n",
        "                anim = \"Animate\"\n",
        "            else:\n",
        "                anim = \"O\"\n",
        "            index += 1\n",
        "\n",
        "        print(\"TOKEN: \" + i.ljust(10) + \"SRL: \" + j.ljust(10) + \"\\t ANIMACY: \" + anim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmFjDhTP_Hfc"
      },
      "source": [
        "## New Sentence Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTJ9VwUS_aKW"
      },
      "source": [
        "\"When you play the game of thrones, you win or you die\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vtgxta-z_cln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12c534ad-18ed-419e-f15b-799620df2f7f"
      },
      "source": [
        "classifier_final(pre_one_liners[0],one_liners[0][0],one_liners[0][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOKEN: When      SRL: B-ARGM-TMP\t ANIMACY: O\n",
            "TOKEN: you       SRL: B-ARG0    \t ANIMACY: Animate\n",
            "TOKEN: play      SRL: O         \t ANIMACY: O\n",
            "TOKEN: the       SRL: B-ARG1    \t ANIMACY: O\n",
            "TOKEN: game      SRL: I-ARG1    \t ANIMACY: O\n",
            "TOKEN: of        SRL: I-ARG1    \t ANIMACY: O\n",
            "TOKEN: throne    SRL: I-ARG1    \t ANIMACY: O\n",
            "TOKEN: ##s       SRL: PAD       \t ANIMACY: O\n",
            "TOKEN: ,         SRL: O         \t ANIMACY: O\n",
            "TOKEN: you       SRL: O         \t ANIMACY: Animate\n",
            "TOKEN: win       SRL: O         \t ANIMACY: O\n",
            "TOKEN: or        SRL: O         \t ANIMACY: O\n",
            "TOKEN: you       SRL: O         \t ANIMACY: Animate\n",
            "TOKEN: die       SRL: O         \t ANIMACY: O\n",
            "TOKEN: .         SRL: O         \t ANIMACY: O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IXib2tcCoiH"
      },
      "source": [
        "\"A man with no motive is a man no one suspects.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2326fJFCk6r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "283ecd78-bdae-4aa8-ca15-02809f331c83"
      },
      "source": [
        "classifier_final(pre_one_liners[1],one_liners[1][0],one_liners[1][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOKEN: A         SRL: O         \t ANIMACY: O\n",
            "TOKEN: man       SRL: O         \t ANIMACY: Animate\n",
            "TOKEN: with      SRL: O         \t ANIMACY: O\n",
            "TOKEN: no        SRL: O         \t ANIMACY: O\n",
            "TOKEN: motive    SRL: O         \t ANIMACY: O\n",
            "TOKEN: is        SRL: O         \t ANIMACY: O\n",
            "TOKEN: a         SRL: B-ARG1    \t ANIMACY: O\n",
            "TOKEN: man       SRL: I-ARG1    \t ANIMACY: Animate\n",
            "TOKEN: no        SRL: B-ARG0    \t ANIMACY: O\n",
            "TOKEN: one       SRL: I-ARG0    \t ANIMACY: O\n",
            "TOKEN: suspects  SRL: O         \t ANIMACY: Animate\n",
            "TOKEN: .         SRL: O         \t ANIMACY: O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6H_ck1XCYry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b59d5c0-365d-4ebe-9ec8-1ccd71b8cf70"
      },
      "source": [
        "classifier_final(pre_one_liners[1],one_liners[1][0],one_liners[1][2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOKEN: A         SRL: B-ARG0    \t ANIMACY: O\n",
            "TOKEN: man       SRL: I-ARG0    \t ANIMACY: Animate\n",
            "TOKEN: with      SRL: I-ARG0    \t ANIMACY: O\n",
            "TOKEN: no        SRL: I-ARG0    \t ANIMACY: O\n",
            "TOKEN: motive    SRL: I-ARG0    \t ANIMACY: O\n",
            "TOKEN: is        SRL: O         \t ANIMACY: O\n",
            "TOKEN: a         SRL: B-ARG1    \t ANIMACY: O\n",
            "TOKEN: man       SRL: I-ARG2    \t ANIMACY: Animate\n",
            "TOKEN: no        SRL: I-ARG2    \t ANIMACY: O\n",
            "TOKEN: one       SRL: I-ARG0    \t ANIMACY: O\n",
            "TOKEN: suspects  SRL: I-ARG0    \t ANIMACY: Animate\n",
            "TOKEN: .         SRL: O         \t ANIMACY: O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdCNOHZZCdVM"
      },
      "source": [
        " \"Now he realised the cruelty of his gift\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRp8tbdtCdZ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1e42564-dc63-4ed8-fef3-de826dd472f5"
      },
      "source": [
        "classifier_final(pre_one_liners[2],one_liners[2][0],one_liners[2][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOKEN: Now       SRL: B-ARGM-TMP\t ANIMACY: O\n",
            "TOKEN: he        SRL: B-ARG0    \t ANIMACY: Animate\n",
            "TOKEN: realised  SRL: O         \t ANIMACY: O\n",
            "TOKEN: the       SRL: B-ARG1    \t ANIMACY: O\n",
            "TOKEN: cruelty   SRL: I-ARG1    \t ANIMACY: O\n",
            "TOKEN: of        SRL: I-ARG1    \t ANIMACY: O\n",
            "TOKEN: his       SRL: I-ARG1    \t ANIMACY: Animate\n",
            "TOKEN: gift      SRL: I-ARG1    \t ANIMACY: O\n",
            "TOKEN: .         SRL: O         \t ANIMACY: O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wymQgo3QCddn"
      },
      "source": [
        "\"Even the smallest person can change the course of the future.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vJJfNY9CYxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5668cbd-9610-4fa2-86c1-63ed2cba637e"
      },
      "source": [
        "classifier_final(pre_one_liners[3],one_liners[3][0],one_liners[3][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOKEN: Even      SRL: B-ARGM-DIS\t ANIMACY: O\n",
            "TOKEN: the       SRL: B-ARG0    \t ANIMACY: O\n",
            "TOKEN: smallest  SRL: I-ARG0    \t ANIMACY: O\n",
            "TOKEN: person    SRL: I-ARG0    \t ANIMACY: Animate\n",
            "TOKEN: can       SRL: B-ARGM-MOD\t ANIMACY: O\n",
            "TOKEN: change    SRL: O         \t ANIMACY: O\n",
            "TOKEN: the       SRL: B-ARG1    \t ANIMACY: O\n",
            "TOKEN: course    SRL: I-ARG1    \t ANIMACY: O\n",
            "TOKEN: of        SRL: I-ARG1    \t ANIMACY: O\n",
            "TOKEN: the       SRL: I-ARG1    \t ANIMACY: O\n",
            "TOKEN: future    SRL: I-ARG1    \t ANIMACY: O\n",
            "TOKEN: .         SRL: O         \t ANIMACY: O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAOaUvujCY1s"
      },
      "source": [
        "\"No other country has counted so many deaths in the pandemic.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qg5buRM1CY5G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dee3e248-8d0a-40cf-e1d2-3971cc06e609"
      },
      "source": [
        "classifier_final(pre_one_liners[4],one_liners[4][0],one_liners[4][1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOKEN: No        SRL: B-ARG0    \t ANIMACY: O\n",
            "TOKEN: other     SRL: I-ARG0    \t ANIMACY: O\n",
            "TOKEN: country   SRL: I-ARG0    \t ANIMACY: O\n",
            "TOKEN: has       SRL: O         \t ANIMACY: O\n",
            "TOKEN: counted   SRL: O         \t ANIMACY: O\n",
            "TOKEN: so        SRL: B-ARG1    \t ANIMACY: O\n",
            "TOKEN: many      SRL: I-ARG1    \t ANIMACY: O\n",
            "TOKEN: deaths    SRL: I-ARG1    \t ANIMACY: O\n",
            "TOKEN: in        SRL: I-ARG1    \t ANIMACY: O\n",
            "TOKEN: the       SRL: I-ARG1    \t ANIMACY: O\n",
            "TOKEN: pan       SRL: I-ARG1    \t ANIMACY: O\n",
            "TOKEN: ##de      SRL: PAD       \t ANIMACY: O\n",
            "TOKEN: ##mic     SRL: PAD       \t ANIMACY: O\n",
            "TOKEN: .         SRL: O         \t ANIMACY: O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67yYmjkmCY-Q"
      },
      "source": [
        "\"A talking tree sits on a bench and looks at a dog.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQr9pP0fCZEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96f26f95-ee42-4250-e88c-402ee1a9a917"
      },
      "source": [
        "classifier_final(pre_one_liners[5],one_liners[5][0],one_liners[5][1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOKEN: A         SRL: B-ARG0    \t ANIMACY: O\n",
            "TOKEN: talking   SRL: I-ARG0    \t ANIMACY: O\n",
            "TOKEN: tree      SRL: I-ARG0    \t ANIMACY: O\n",
            "TOKEN: sits      SRL: O         \t ANIMACY: O\n",
            "TOKEN: on        SRL: O         \t ANIMACY: O\n",
            "TOKEN: a         SRL: O         \t ANIMACY: O\n",
            "TOKEN: bench     SRL: O         \t ANIMACY: O\n",
            "TOKEN: and       SRL: O         \t ANIMACY: O\n",
            "TOKEN: looks     SRL: O         \t ANIMACY: O\n",
            "TOKEN: at        SRL: O         \t ANIMACY: O\n",
            "TOKEN: a         SRL: B-ARG1    \t ANIMACY: O\n",
            "TOKEN: dog       SRL: I-ARG1    \t ANIMACY: Animate\n",
            "TOKEN: .         SRL: O         \t ANIMACY: O\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}